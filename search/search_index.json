{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Solutions to Introduction to Algorithms Fourth Edition","text":""},{"location":"#refference","title":"Refference","text":"<ul> <li>Introduction to Algorithms Solution Third Edition.</li> </ul>"},{"location":"#more-information","title":"More Information","text":"<p>For a clear commit history, I rebase my repository regularly. Therefore, if you have forked the repository before, consider re-forking it again.</p>"},{"location":"#license","title":"License","text":"<p>Licensed under the MIT License.</p>"},{"location":"Chap01/1.1/","title":"1.1 Algorithms","text":""},{"location":"Chap01/1.1/#11-1","title":"1.1-1","text":"<p>Describe your own real-world example that requires sorting. Describe one that requires finding the shortest distance between two points.</p> <ul> <li>Sorting: rankings in games</li> <li>Finding the shortest distance between two points: takeout delivery always requires deliveryman finding the shortest distance</li> </ul>"},{"location":"Chap01/1.1/#11-2","title":"1.1-2","text":"<p>Other than speed, what other measures of efficiency might you need to consider in a real-world setting?</p> <p>Memory efficiency and coding efficiency.</p>"},{"location":"Chap01/1.1/#11-3","title":"1.1-3","text":"<p>Select a data structure that you have seen, and discuss its strengths and limitations.</p> <p>arrays:</p> <ul> <li>Strengths: random access.</li> <li>Limitations: push, pop, insertion and deletion.</li> </ul>"},{"location":"Chap01/1.1/#11-4","title":"1.1-4","text":"<p>How are the shortest-path and traveling-salesperson problems given above similar?How are they different?</p> <ul> <li>Similar: finding shortest-path.</li> <li>Different: the latter traves every points and returns.</li> </ul>"},{"location":"Chap01/1.1/#11-5","title":"1.1-5","text":"<p>Suggest a real-world problem in which only the best solution will do. Then come up with one in which \"approximately\" the best solution is good enough.</p> <ul> <li>Best: sovling an equaion.</li> <li>Aooroximately: find a solution for which an inequality holds.</li> </ul>"},{"location":"Chap01/1.1/#11-6","title":"1.1-6","text":"<p>Describe a real-world problem in which sometimes the entire input is available before you need to solve the problem, but other times the input is not entirely available in advance and arrives over time.</p> <p>In team work:</p> <ul> <li>Available: if teammates finish their work in time</li> <li>Not avalibale: teammates not finish their work you need in time</li> </ul>"},{"location":"Chap01/1.2/","title":"1.2 Algorithms as a technology","text":""},{"location":"Chap01/1.2/#12-1","title":"1.2-1","text":"<p>Give an example of an application that requires algorithmic content at the application level, and discuss the function of the algorithms involved.</p> <ul> <li>Application: Class schedule</li> <li>Function: avoid confict that a student has two classes at same time or a student has a class which requires completion one unfinished training course</li> </ul>"},{"location":"Chap01/1.2/#12-2","title":"1.2-2","text":"<p>Suppose we are comparing implementations of insertion sort and merge sort on the same machine. For inputs of size $n$ , insertion sort runs in $8n^2$ steps, while merge sort runs in $64n\\lg n$ steps. For which values of $n$ does insertion sort beat merge sort?</p> <p>$$ \\begin{aligned} 8n^2 &lt; 64n \\lg n \\cr 0 &lt; n &lt; 8 \\lg n \\cr 1&lt; 2^n &lt; n^8 \\cr 2 \\leq n \\leq 43 \\cr \\end{aligned} $$</p>"},{"location":"Chap01/1.2/#12-3","title":"1.2-3","text":"<p>What is the smallest value of n such that an algorithm whose running time is $100n^2$ runs faster than an algorithm whose running time is $2^n$ on the same machine?</p> <p>$$ \\begin{aligned} 100n^2 &lt; 2^n \\cr n \\ge 15.\\cr \\end{aligned} $$</p>"},{"location":"Chap01/Problems/1-1/","title":"1-1 Comparison of running times","text":""},{"location":"Chap01/Problems/1-1/#1-1","title":"1-1","text":"<p>For each function $f(n)$ and time $t$ in the following table, determine the largest size $n$ of a problem that can be solved in time $t$, assuming that the algorithm to solve the problem takes $f(n)$ microseconds.</p> Function 1 second 1 minute 1 hour 1 day 1 month 1 year 1 century $lg n$ $2^{10^6}$ $2^{6 \\times 10^7}$ $2^{3.6 \\times 10^9}$ $2^{8.64 \\times 10^{10}}$ $2^{2.59 \\times 10^{12}}$ $2^{3.15 \\times 10^{13}}$ $2^{3.15 \\times 10^{15}}$ $\\sqrt n$ $10^{12}$ $3.6 \\times 10^{15}$ $1.3 \\times 10^{19}$ $7.46 \\times 10^{21}$ $6.72 \\times 10^{24}$ $9.95 \\times 10^{26}$ $9.95 \\times 10^{30}$ $n$ $10^6$ $6 \\times 10^7$ $3.6 \\times 10^9$ $8.64 \\times 10^{10}$ $2.59 \\times 10^{12}$ $3.15 \\times 10^{13}$ $3.15 \\times 10^{15}$ $n lg n$ $6.24 \\times 10^4$ $2.8 \\times 10^6$ $1.33 \\times 10^8$ $2.76 \\times 10^9$ $7.19 \\times 10^{10}$ $7.98 \\times 10^{11}$ $6.86 \\times 10^{13}$ $n^2$ 1,000 7,745 60,000 293,938 1,609,968 5,615,692 56,156,922 $n^3$ 100 391 1,532 4,420 13,736 31,593 146,645 $2^n$ 19 25 31 36 41 44 51 $n!$ 9 11 12 13 15 16 17"},{"location":"Chap02/2.1/","title":"2.1 Insertion sort","text":""},{"location":"Chap02/2.1/#21-1","title":"2.1-1","text":"<p>Using Figure 2.2 as a model, illustrate the operation of $\\text{INSERTION-SORT}$ on the array $A = \\langle 31, 41, 59, 26, 41, 58 \\rangle$.</p> <p></p> <p>as shown in the figure above, the array A change as follow $$ \\begin{aligned} A = \\langle 31, 41, 59, 26, 41, 58 \\rangle\\cr A = \\langle 31, 41, 59, 26, 41, 58 \\rangle\\cr A = \\langle 26, 31, 41, 59, 41, 58 \\rangle\\cr A = \\langle 26, 31, 41, 41, 59, 58 \\rangle\\cr A = \\langle 26, 31, 41, 41, 58, 59 \\rangle\\cr A = \\langle 26, 31, 41, 41, 58, 59 \\rangle\\cr \\end{aligned} $$</p>"},{"location":"Chap02/2.1/#21-2","title":"2.1-2","text":"<p>Consider the procedure SUM-ARRAY on the facing page. It computes the sum of the n numbers in array $A[1:n]$. State a loop invariant for this procedure, and use its initialization, maintenance, and termination properties to show that the SUMARRAY procedure returns the sum of the numbers in $A[1:n]$.</p> <pre><code>SUM-ARRAY(A,n)\nsum = 0\nfor i = 1 to n\n  sum = sum + A[i]\nreturn sum\n</code></pre> <ul> <li>Loop invariant: At the start of the ith iteration of the for loop, the equation $sum = \\sum_{j=1}^{i-1} A[j]$ holds.</li> <li>Initialization: Before the first iteration (i=1), $sum = \\sum_{j=1}^{0} A[j] = 0$ holds.</li> <li>Maintenance: During the ith iteration, <code>sum = sum + A[i]</code> results $sum = \\sum_{j=1}^{i-1} A[j] + A[i] = \\sum_{j=1}^{i} A[j]$, at the end of this iteration, Incrementing i for the next iteration of the for loop then preserves the loop invariant.</li> <li>Termination: The loop terminates when $i &gt; n$, since i increase 1 at each iteration, $i=n+1$ at termination.$sum = \\sum_{j=1}^{i-1} A[j]=\\sum_{j=1}^{n} A[j]$ holds.</li> </ul>"},{"location":"Chap02/2.1/#21-3","title":"2.1-3","text":"<p>Rewrite the INSERTION-SORT procedure to sort into monotonically decreasing instead of monotonically increasing order.</p> <pre><code>INSERTION_SORT_NONINCEASE(A)\n  for i=2 to A.length()\n    key = A[i]\n    j = i -1\n    while j &gt;=1 and A[j] &gt; key\n      A[j+1] = A[j]\n    A[j+1] = key \n</code></pre>"},{"location":"Chap02/2.1/#21-4","title":"2.1-4","text":"<p>Consider the searching problem:</p> <p>Input: A sequence of n numbers $\\langle a1 , a2,..., an \\rangle$stored in array A[1:n] and a value x.</p> <p>Output: An index i such that x equals $A[i]$ or the special value NIL if x does not appear in $A$.</p> <p>Write pseudocode for linear search, which scans through the array from beginning to end, looking for x. Using a loop invariant, prove that your algorithm is correct. Make sure that your loop invariant fulfills the three necessary properties.</p> <pre><code>LINEAR_SEARCH (A,x)\n  for i = 1 to A.length()\n    if A[i] == x \n      return i\n    return NIL\n</code></pre> <ul> <li>Loop invariant: At the start of the ith iteration of the for loop, no element in the subarray $A[1:i-1] equal $x$</li> <li>Initialization: Before the first iteration (i=1), $A[1:0]$ is empty, so no elements in $A[1:0]$ eqaul $x$</li> <li>Maintenance: During the ith iteration, if $A[i]==x$ , the algorithms return the correct answer $i$.Otherwise, i increases $1$ and no element in $A[1:i]$ equal $x$, at the start of the ith iteration. Incrementing i for the next iteration of the for loop then preserves the loop invariant.</li> <li>Termination: The loop terminates when $i &gt; n$, since i increase 1 at each iteration, $i=n+1$ at termination. No element in $A[1:i-1]=A[1:n]$ equal $x$.</li> </ul>"},{"location":"Chap02/2.1/#21-5","title":"2.1-5","text":"<p>Consider the problem of adding two n-bit binary integers $a$ and $b$, stored in two $n$-element arrays $A[0:n-1]$ and $B[0:n-1]$, where each element is either $0$ or $1$. $a = \\sum_{i=0}^{n-1} A[i] \\cdot 2^i$, and $b = \\sum_{i=0}^{n-1} B[i] \\cdot 2^i$. The sum $c = a + b$ of the two integers should be stored in binary form in an ($n+1$)-element array $C[0:n]$, where $c = \\sum_{i=0}^{n} C[i] \\cdot 2^i$. Write a procedure ADD-BINARY-INTEGERS that takes as input arrays $A$ and $B$, along with the length n, and returns array C holding the sum.</p> <pre><code>ADD_BINARY_INTEGERS(A,B,n)\n    C[0:n]\n    carry=0\n    for i = 1 to n\n        C[i-1]=(A[i]+B[i]+carry)%2\n        carry =(A[i]+B[i]+carry)/2\n    C[n] = carry\n    return C;\n</code></pre>"},{"location":"Chap02/2.2/","title":"2.2 Analyzing algorithms","text":""},{"location":"Chap02/2.2/#22-1","title":"2.2-1","text":"<p>Express the function $n^3/1000+100n^2-100n+3$ in terms of $\\Theta$-notaion.</p> <p>$\\Theta (n^3)$</p>"},{"location":"Chap02/2.2/#22-2","title":"2.2-2","text":"<p>Consider sorting $n$ numbers stored in array $A[1:n]$ by first finding the smallest element of $A[1:n]$ and exchanging it with the element in $A[1]$. Then find the smallest element of $A[2:n]$, and exchange it with $A[2]$. Then find the smallest element of $A[3:n]$, and exchange it with $A[3]$. Continue in this manner for the first $n-1$ elements of $A$. Write pseudocode for this algorithm, which is known as selection sort. What loop invariant does this algorithm maintain? Why does it need to run for only the first $n-1$ elements, rather than for all $n$ elements? Give the worst-case running time of selection sort in $\\Theta$-notation. Is the best-case running time any better?</p> <ul> <li>pseudocode</li> </ul> <pre><code>SELECTION_SORT(A)\n    for i = 1 to A.length()-1\n        minindex = i\n        for j = i to A.length()\n            if A[j]&lt;A[minindex]\n                minindex = j\n        swap(A[i],A[minindex]) \n</code></pre> <ul> <li>loop invariant: At the start of $i$th iteration, $A[1:i-1]$ consist of the $i-1$ elements of $A[1:n]$ and it is at sort order.</li> <li>When the i-1 smallest elements are sort in the subarray $A[1:n-1], the biggest element must be $A[n]$, and then A[1:n] are sort.</li> <li>worse case: in each iteration, $j$ increase from $i$ to $n$, $running time = (n) + (n-1) + ... + (2) = \\Theta (n^2)$</li> <li>best case: the same as worst case since in each iteration in inner for loop, j must go from i to n to get the minindex.</li> </ul>"},{"location":"Chap02/2.2/#22-3","title":"2.2-3","text":"<p>Consider linear search again (see Exercise 2.1-4). How many elements of the input array need to be checked on the average, assuming that the element being searched for is equally likely to be any element in the array? How about in the worst case?</p> <p>Using $\\Theta$-notation, give the average-case and worst-case running times of linear search. Justify your answers.</p> <ul> <li>average-case: $\\text{running time}\\sum_{i=1}^{n} i/n = (n+1)/2 = \\Theta(n)$</li> <li>worst-case: $\\text{running time} = n =\\Theta(n)$</li> </ul>"},{"location":"Chap02/2.2/#22-4","title":"2.2-4","text":"<p>How can you modify any sorting algorithm to have a good best-case running time</p> <p>modidy combine with insertion sort ,such as check whether each element in array is smaller(bigger) than the righthand element, since it produce a best - case running time of $\\Theta(n)$, the same as travel throught the array, which is a neccessary step for sorting.</p>"},{"location":"Chap02/2.3/","title":"2.3 Designing algorithms","text":""},{"location":"Chap02/2.3/#23-1","title":"2.3-1","text":"<p>Using Figure 2.4 as a model, illustrate the operation of merge sort on an array initially containing the sequence $\\langle 3, 41, 52, 26, 38, 57, 9, 49 \\rangle$.</p> <p>$$ [3|41|52|26|38|57|9|49]$$</p> <p>$$\\downarrow$$</p> <p>$$[3|41|52|26] \\quad [38|57|9|49]$$</p> <p>$$\\downarrow$$</p> <p>$$[3|41] \\quad [52|26] \\quad [38|57] \\quad [9|49]$$</p> <p>$$\\downarrow$$</p> <p>$$[3] \\quad [41] \\quad [52] \\quad [26] \\quad [38] \\quad [57] \\quad [9] \\quad [49]$$</p> <p>$$\\downarrow$$</p> <p>$$[3|41] \\quad [26|52] \\quad [38|57] \\quad [9|49]$$</p> <p>$$\\downarrow$$</p> <p>$$[3|26|41|52] \\quad [9|38|49|57]$$</p> <p>$$\\downarrow$$</p> <p>$$[3|9|26|38|41|49|52|57]$$</p>"},{"location":"Chap02/2.3/#23-2","title":"2.3-2","text":"<p>The test in line 1 of the MERGE-SORT procedure reads \"if $p \\geq r$\" rather than \"if $p \\neq r$.\" If MERGE-SORT is called with $p &gt; r$, then the subarray $A[p:r]$ is empty. Argue that as long as the initial call of MERGE-SORT(A, 1, n) has $n \\geq 1$, the test \"if $p \\neq r$\" suffices to ensure that no recursive call has $p &gt; r$.</p> <ul> <li>inductive case:</li> </ul> <p>$$ \\begin{aligned} if \\quad p \\leq r\\cr if \\quad p &lt; r\\cr q = \\lfloor (p+r)/2 \\rfloor\\cr q \\geq p\\cr q+1 \\leq r\\cr MERGE-SORT(A,p,q)\\cr MERGE-SORT(A,q+1,r)\\cr \\text{else return}\\cr \\text{no recursive call has p&gt;r}\\cr p \\leq r \\text{ holds in new recursive call}\\cr \\end{aligned} $$</p> <ul> <li>base case:</li> </ul> <p>$$ \\begin{aligned} MERGE-SORT(A,1,n)\\cr n \\geq 1\\cr p \\leq r holds\\cr \\end{aligned} $$</p> <p>\"if $p \\neq r$\" suffices to ensure that no recursive call has $p &gt; r$.</p>"},{"location":"Chap02/2.3/#23-3","title":"2.3-3","text":"<p>State a loop invariant for the while loop of lines 12\u201318 of the MERGE procedure. Show how to use it, along with the while loops of lines 20\u201323 and 24\u201327, to prove that the MERGE procedure is correct.</p> <ul> <li> <p>loop invariant: at the start of each iteration, $A[p:k-1]$ consist of the smallest elements of union of $L$ and $R$ at sort order.</p> </li> <li> <p>when the while loop of lines 12-18 finish $A[p:k-1]$ consist of the smallest elements of union of $L$ and $R$ at sort order, either while loop of lines 20\u201323 or 24\u201327 will be run, $L$ or $R$ has the biggest rest elements in sort order.when the second while loop finish, they are added to the end of A, and A consist of all elements at sort order.</p> </li> </ul>"},{"location":"Chap02/2.3/#23-4","title":"2.3-4","text":"<p>Use mathematical induction to show that when $n \\geq 2$ is an exact power of 2, the solution of the recurrence</p> <p>$$ \\begin{aligned} T(n) = \\begin{cases} 2 &amp; \\text{if } n = 2, \\cr 2T(n/2) + n &amp; \\text{if } n &gt; 2 \\end{cases} \\end{aligned} $$ is $T(n) = n \\lg n$.</p> <p>$$ \\begin{aligned}     T(n) &amp; = T(2^t)\\cr     &amp; = 2T(2^{t-1}) + 2^t\\cr     &amp; = 4T(2^{t-2}) + 2\\cdot{2^t}\\cr     &amp; = 2^{t-1}T(2) + 2^t\\cdot{\\lg 2^t }\\cr     &amp; = n\\lg n \\end{aligned} $$</p>"},{"location":"Chap02/2.3/#23-5","title":"2.3-5","text":"<p>You can also think of insertion sort as a recursive algorithm. In order to sort $A[1:n]$, recursively sort the subarray $A[1:n-1]$ and then insert $A[n]$ into the sorted subarray $A[1:n-1]$. Write pseudocode for this recursive version of insertion sort. Give a recurrence for its worst-case running time.</p> <pre><code>INSERTIONSORT_RECURXIVE(A,n)\n    if n = 1\n        return\n    INSERTIONSORT_RECURXIVE(A,n-1)\n    i = n-1\n    key = A[n]\n    while A[i]&gt;key and i&gt;=1\n        A[i+1]=A[i]\n        i--\n    A[i+1]=key\n</code></pre>"},{"location":"Chap02/2.3/#23-6","title":"2.3-6","text":"<p>Referring back to the searching problem (see Exercise 2.1-4), observe that if the subarray being searched is already sorted, the searching algorithm can check the midpoint of the subarray against v and eliminate half of the subarray from further consideration. The binary search algorithm repeats this procedure, halving the size of the remaining portion of the subarray each time. Write pseudocode, either iterative or recursive, for binary search. Argue that the worst-case running time of binary search is $\\Theta (\\lg n)$. iterative:</p> <pre><code>BINARY_SEARCH_ITERATIVE(A,l,r,x)\n    while r&gt;=l \n        mid = (l+r)/2\n        if A[mid] == x\n            return mid\n        else if A[mid] &gt; x\n            r = mid -1\n        else\n            l = mid +1\n    return NIL\n</code></pre> <p>recursive:</p> <pre><code>BINARY_SEARCH_RECURVISE(A,l,r,x)\n    if(l&gt;r)\n        return NIL\n    mid= (l+r)/2\n    if A[mid] == x\n        return x\n    if A[mid] &lt; x\n        return A[A,mid+1,r,x]\n    if A[mid] &gt; x\n        return A[A,l,mid-1,x]\n</code></pre> <p>the worst-case is no x in $A$, BINARY SEARCH will end al condition l&gt;r which occurs when A[l:r] is empty. since BINARY SEARCH halving the size of A[l:r] each time, it cost $\\Theta (\\lg n)$ to make A[l:r] empty</p>"},{"location":"Chap02/2.3/#23-7","title":"2.3-7","text":"<p>The while loop of lines 5-7 of the INSERTION-SORT procedure in Section 2.1 uses a linear search to scan (backward) through the sorted subarray $A[1:j-1]$. What if insertion sort used a binary search (see Exercise 2.3-6) instead of a linear search? Would that improve the overall worst-case running time of insertion sort to $\\Theta(n \\lg n)$</p> <p>It can save search time, but can not save movement time. At worst -case, it cost $j-1 = \\Theta(j)$ time  to move elements bigger than $A[j]$ in $A[1:j-1]$.The same as the original INSERTION-SORT Therefore, that improve the overall worst-case running time of insertion sort to $\\Theta(n\\lg n)$</p>"},{"location":"Chap02/2.3/#23-8","title":"2.3-8","text":"<p>Dscribe Describe an algorithm that, given a set $S$ of $n$ integers and another integer $x$, determines whether $S$ contains two elements that sum to exactly $x$. Your algorithm should take $\\Theta (n \\lg n)$ time in the worst case.</p> <pre><code>FUCTION(S,x)\n    n=S.lenfth()\n    MERGE_SORT(S,1,n) //cost $\\Theta n \\lgn$\n    i=1\n    j=n\n    sum=A[i]+A[j]\n    while sum !=x and j&gt;i //cost \\Theta n\n        if sum &gt; x \n            j--\n        else i++ \n    if sum == x\n        return i j\n    else\n        return NIL\n</code></pre> <p>The time complexity of the algorithm is $\\Theta (n \\lg n)+\\Theta (n)$</p>"},{"location":"Chap02/Problems/2-1/","title":"2-1 Insertion sort on small arrays in merge sort","text":"<p>Although merge sort runs in $\\Theta(n \\lg n)$ worst-case time and insertion sort runs in $\\Theta (n^2)$ worst-case time, the constant factors in insertion sort can make it faster in practice for small problem sizes on many machines. Thus it makes sense to coarsen the leaves of the recursion by using insertion sort within merge sort when subproblems become sufficiently small. Consider a modification to merge sort in which $n/k$ sublists of length $k$ are sorted using insertion sort and then merged using the standard merging mechanism, where $k$ is a value to be determined.</p> <p>a. Show that insertion sort can sort the $n/k$ sublists, each of length k, in $\\Theta (nk)$ worst-case time.</p> <p>b. Show how to merge the sublists in $\\Theta (n \\lg (n/k))$ worst-case time.</p> <p>c. Given that the modified alogotithm runs in $\\Theta (nk+n \\lg (n/k))$ worst-case time, what is the largest value of k as a function of n for which the modified algorithm has the same running time as standard merge sort, in terms of $\\Theta$-notation?</p> <p>d. How should you choose k in practice?</p>"},{"location":"Chap02/Problems/2-1/#a","title":"a","text":"<p>Each sublists of length can be sorted in $\\Theta (k^2)$, there are n/k sublists, so in n $n/k \\cdot \\Theta(k^2) = \\Theta (nk)$ sorst-case time.</p>"},{"location":"Chap02/Problems/2-1/#b","title":"b","text":"<p>Each level of merging need to merge $n$ elements, and there are $\\lg (n/k)$levels(since we merge each two sublists in one in each level). So the worse-case time to merge the sublists is $\\Theta (n \\lg (n/k))$.</p>"},{"location":"Chap02/Problems/2-1/#c","title":"c","text":"<p>$$ \\begin{aligned}   &amp;  if \\quad \\Theta (nk+n \\lg (n/k)) = \\Theta (n \\lg n)\\cr   &amp; \\exist  \\quad c_{1} , c_{2} , n_{0} \\quad satisfies:\\cr   &amp;  c_{1}(n \\lg n) \\leq nk+n \\lg (n/k) \\leq c_{2}(n \\lg n) \\quad for \\quad n &gt; n_{0}\\cr     &amp;(c_{1}-1)(\\lg n )\\leq k - \\lg k \\leq (c_{2}-1)(\\lg n ) \\quad for \\quad n &gt; n_{0}\\cr    &amp; since \\quad lg k \\llless k, \\quad \\text{we have } k = \\Theta (\\lg n)\\cr \\end{aligned} $$</p>"},{"location":"Chap02/Problems/2-1/#d","title":"d","text":"<p>Choose k be the largest size of the sublists that insertion-sort is faster than merge-sort.</p>"},{"location":"Chap02/Problems/2-2/","title":"2-2 Correctness of bubblesort","text":"<p>Bubblesort is a popular, but inefficient, sorting algorithm. It works by repeatedly swapping adjacent elements that are out of order. The procedure BUBBLESORT sorts array $A[1:n]$.</p> <pre><code>BUBBLESORT(A,n)\n  for i = 1 to n - 1\n      for j = n downto i + 1\n          if A[j] &lt; A[j-1]\n              exchange A[j] with A[j-1]\n</code></pre> <p>a. Let $A'$ denote the array $A$ after BUBBLESORT&gt; (A, n) is executed. To prove that it terminates and that $$ A'[1] \\leq A'[2] \\leq \\cdots \\leq A'[n] $$ In order to show that BUBBLESORT actually sorts, what else do you need to prove?</p> <p>The next two parts prove inequality (2.5).</p> <p>b. State precisely a loop invariant for the for loop in lines 2-4, and prove that this loop invariant holds. Your proof should use the structure of the loop-nvariant proof presented in this chapter.</p> <p>c. Using the termination condition of the loop invariant proved in part (b), state a loop invariant for the for loop in lines 1-4 that allows you to prove inequal ity (2.5). Your proof should use the structure of the loop-invariant proof presented in this chapter.</p> <p>d. What is the worst-case running time of BUBBLESORT? How does it compare with the running time of INSERTION-SORT?</p>"},{"location":"Chap02/Problems/2-2/#a","title":"a","text":"<p>$A'$ consist of all the elements of $A$</p>"},{"location":"Chap02/Problems/2-2/#b","title":"b","text":"<p>loop invariant: At the start of each iteration of $for$ loop in lines 2-4, $A[j:n]$ consist of the elements originally in $A[j:n]$ before entering the loop but possibly in a different order, and $A[j]$ is the smallest one of them.</p> <p>Initialization: $j=n$, $A[n]$ consist of the elements originally in A$[n]$ and it is the smallest.</p> <p>Maintenance: During the iteration, if$A[j-1]&lt;A[j]$, we swap them. So A[j-1] will be the smallest and A[j-1:n] consist of the elements originally in $A[j-1:n]$. Decresementing j preserves the loop invariant.</p> <p>Termination: the loop termination when $j = i$, $A[i:n]$ consist of the elements originally in $A[i:n]$ before entering the loop but possibly in a different order, and $A[i]$ is the smallest one of them.</p>"},{"location":"Chap02/Problems/2-2/#c","title":"c","text":"<p>Loop invariant: At the start of each iteration of the for loop of lines 1-4, the subarray $A[1..i \u2212 1]$ consists of the $i - 1$ smallest elements in $A[1..n]$ in sorted order. $A[i..n]$ consists of the $n - i + 1$ remaining elements in $A[1..n]$.</p> <p>Initialization: Initially the subarray $A[1..i \u2212 1]$ is empty and trivially this is the smallest element of the subarray.</p> <p>Maintenance: From part (b), after the execution of the inner loop, $A[i]$ will be the smallest element of the subarray $A[i..n]$. And in the beginning of the outer loop, $A[1..i \u2212 1]$ consists of elements that are smaller than the elements of $A[i..n]$, in sorted order. So, after the execution of the outer loop, subarray $A[1..i]$ will consists of elements that are smaller than the elements of $A[i + 1..n]$, in sorted order.</p> <p>Termination: The loop terminates when $i = A.length$. At that point the array $A[1..n]$ will consists of all elements in sorted order.</p>"},{"location":"Chap02/Problems/2-2/#d","title":"d","text":"<p>The $i$th iteration of the for loop of lines 1-4 will cause $n \u2212 i$ iterations of the for loop of lines 2-4, each with constant time execution, so the worst-case running time of bubble sort is $\\Theta(n^2)$ which is same as the worst-case running time of insertion sort.</p>"},{"location":"Chap02/Problems/2-3/","title":"2-3 Correctness of Horner\u2019s rule","text":"<p>You are given the coefficents $a_0; a_1; a_2; \\dots; a_n$ of a polynomial $$ \\begin{aligned}     P(x) &amp; = \\sum_{k=0}^n a_kx^k \\cr     &amp;=a_0+a_1x+a_2x^2+\\cdots +a_{n-1}x^{n-1}+a_nx^n, \\end{aligned} $$ and you want to evaluate this polynomial for a given value of x. Horner's rule says to evalusate the polynomial according to this &gt; parenthesization: $$ P(x) = a_0 + x ( a_1 + x ( a_2 + \\cdots + x ( a_{n-1} + x a_n) &gt; \\cdots )). $$ The procedure HORNER implements Horner**\u2019s rule to &gt; evaluate $P(x)$, &gt; givem the coefficients $a_0,a_1,a_2,\\dots,&gt; a_n$ in an array $A[0:n]$ and &gt; the value of $x$.</p> <pre><code>HORNER (A,n,x)\n    p=0\n    for i = n downto 0\n        p = A[i] + x * p\n    return p\n</code></pre> <p>a. In terms of $\\Theta$-notation, what is the tunning time of this procedure?</p> <p>b. Write pseudocode to implement the naive polynomial-evaluation algorithm that computes each term of the polynomial from scratch. What is the running time of this algorithm? How does it compare with HORNER?</p> <p>c. Consider the following loop invariant for the procedure HORNER:</p> <p>At the start of each iteration of the for loop of lines 2-3, $$ p = \\sum {k=0}^{n-(i+1)} A[k+i+1] \\cdot x^k $$ Interpret a summation with no terms at equaling 0. Following the structure of th loop-invariant proof presented in this chapter, use this loop invariant to show that, at termination, $p = \\sum {k=0} ^{n} A[k] \\cdot x^k$</p>"},{"location":"Chap02/Problems/2-3/#a","title":"a","text":"<p>$\\Theta(n)$</p>"},{"location":"Chap02/Problems/2-3/#b","title":"b","text":"<p>pseudocode:</p> <pre><code>naive_polynomia_evaluation (A,n,x)\n    p=0\n    for i = 0 to n\n        term = A[i]\n        for j = 1 to i\n            term = term * x\n        p = term + p\n    return p\n</code></pre> <p>$\\text{running time:} \\Theta (n^2)$, HORNER beat it asymptotically.</p>"},{"location":"Chap02/Problems/2-3/#c","title":"c","text":"<p>Initialization: $i=n, p=\\sum_{k=0}^{n-(n+1)} A[k+n+1] \\cdot{x^k} = 0$, loop invariant holds.</p> <p>Maintenance: in the end of each iteration:</p> <p>$$ \\begin{aligned}     p &amp; =  A[i] + x \\cdot{\\sum_{k=0}^{n-(i+1)} A[k+i+1] \\cdot {x^k}} \\cr     &amp; =  A[i] + \\sum_{k=0}^{n-(i+1)} A[k+i+1] \\cdot{x^{k+1}}\\cr     &amp; = A[i] \\cdot{x^0} + \\sum_{k=1}^{n-i} A[k+i] \\cdot{x^{k}}\\cr     &amp; = \\sum_{k=0}^{n-i} A[k+i] \\cdot{x^k}\\cr \\end{aligned} $$</p> <p>Decrementing i preserves the loop invariant.</p> <p>Termination: The loop terminates at $i=\u22121$. If we substitute,</p> <p>$$ p = \\sum_{k=0}^{n-(i+1)} A[k+i+1] \\cdot{x^k} = \\sum_{k=0}^{n} A[k] \\cdot{x^k} $$</p>"},{"location":"Chap02/Problems/2-4/","title":"2-4 Inversions","text":"<p>Let $A[1:n]$ be an array of n distinct numbers. If $i &lt; j$ and $A[i] &gt; A[j]$, then the pair $(i,j)$ is called an inversion of A.</p> <p>a. List the five inversions of the array $\\langle 2,3,8,6,1 \\rangle$.</p> <p>b. What array with elements from the set $\\langle 1,2,\\dots ,n \\rangle$. has the most inversions? How many does it have?</p> <p>c. What is the relationship between the running time of insertion sort and the number of inversions in the input array? Justify your answer.</p> <p>d. Give an algorithm that determines the number of inversions in any permutation. on n elementa in $\\Theta (n \\lg n)$ worse-case time. (Hint: Modify merge sort.)</p> <ul> <li> <p>a. (1,5),(2,5),(3,4),(3,5),(4,5)</p> </li> <li> <p>b. $\\langle n,n-1,\\dots ,1\\rangle \\text{ totally has } {n \\choose 2} = n(n-1)/2 \\text{ inversions}$</p> </li> <li> <p>c. The running time of insertsion sort is a constant times the number of inversions, since each time exchanging elements in insertion sort reduces Inversions num by 1.</p> </li> <li> <p>d.</p> </li> </ul> <pre><code>MERGE_INVERSIONS (A,l,r,mid)\n    inversions = 0\n    n1 = mid - l\n    n2 = r - mid -1\n    L[0:n1]\n    for i = 0 to n1\n        L[i] = A[l + i]\n    R[0:n2]\n    for j = 0 to n2\n        R[j] = A[mid + 1 + j]\n    i=0 \n    j=0\n    k=l\n    while i &lt;= n1 and j &lt;= n2\n        if L[i] &lt;= R[j]\n            A[k] = L[i]\n            i++\n            k++\n        else\n            A[k] = R[j]\n            j++\n            k++\n            inversions + = n1 - i + 1\n    while i &lt;= n1\n        A[k] = L[i]\n        k++\n        i++\n    while j &lt;= n2\n        A[k] = R[j]\n        k++\n        j++\nMERGE_SORT_INVERSIONS(A,l,r)\n    inversions = 0\n    if l &gt;=r\n        return inversions\n    mid = floor((l+r)/2)\n    inversions + = MERGE_SORT_INVERSIONS(A,l,mid)\n    inversions + = MERGE_SORT_INVERSIONS(A,mid+1,r)\n    inversions + = MERGE_INVERSIONS(A,l,mid,r)\n    return inversions\n</code></pre>"},{"location":"Chap03/3.1/","title":"3.1 O-notation, $\\Omega$-notation, and $\\Theta$-notation","text":""},{"location":"Chap03/3.1/#31-1","title":"3.1-1","text":"<p>Modify the lower-bound argument for insertion sort to handle input sizes that are not necessarily a multiple of 3.</p> <p>At least $\\lfloor \\frac n 3 \\rfloor \\geq \\frac 3n-1$ elements have to pass through at least $\\lfloor \\frac n 3 \\rfloor \\geq \\frac 3n-1$ the time taken by INSERTION-SORT in the worst case is at least proportional to $(\\frac 3n-1)(\\frac 3n-1)=\\Omega(n^2)$</p>"},{"location":"Chap03/3.1/#31-2","title":"3.1-2","text":"<p>Using reasoning similar to what we used for insertion sort, analyze the running time of the selection sort algorithm from Exercise 2.2-2.</p> <p>Inner loop in selection sort iterates $(n-i+1)$ times, for i = 1 to n-1, so the running time of selection sort is $\\sum_{i=1}^{n-1} (n-i+1)=(n+2)(n-1)/2=\\Theta(n^2)$</p>"},{"location":"Chap03/3.1/#31-3","title":"3.1-3","text":"<p>Suppose that $\\alpha$ is a fraction in the range $0 &lt; \\alpha &lt; 1$. Show how to generalize the lower-bound argument for insertion sort to consider an input in which the $\\alpha n$ largest values start in the first $\\alpha n$ positions. What additional restriction do you need to put on $\\alpha$? What value of $\\alpha$ maximizes the number of times that the $\\alpha n$ largest values must pass through each of the middle $(1-2 \\alpha)$ array positions?</p> <p>At least $\\alpha n$ values have to pass through at least $(1-2 \\alpha)n$ times. insertion sort is at least proportional to $(\\alpha n)(1-2 \\alpha)n = \\alpha(1-2\\alpha)n^2 = \\Omega(n^2)$ if $\\alpha(1-2\\alpha)=\\Omega(1)$</p> <p>$\\max(\\alpha(1-2\\alpha))=\\frac 1 8$ when $\\alpha = \\frac 1 4$</p>"},{"location":"Chap03/3.2/","title":"3.2Asymptotic notation formal definitions","text":""},{"location":"Chap03/3.2/#32-1","title":"3.2-1","text":"<p>Let $f(n)$ and $g(n)$ be the asympotically nonnegative functions. Using the basic definition of $\\Theta$-notation, prove that max ${f(n),g(n)} = \\Theta (f(n) + g(n))$.</p> <p>$$ \\begin{aligned} \\text{Let } n_0 \\text{ be a value that makes f(n) greater than 0 for n &gt;}n_0\\cr \\text{Then for }n&gt;n_0\\cr \\frac 1 2 (f(n)+g(n)) \\leq \\max(f(n),g(n)) \\leq 1(f(n)+g(n))\\cr \\max(f(n),g(n))=\\Theta(f(n)+g(n))\\cr \\end{aligned} $$</p> <p>Tips: I will omit details like \"$\\text{Let } n_0 \\text{ be a value that makes f(n) greater than 0 for n &gt;}n_0$\"\"$\\exist c,n_0$\" and \"for $n &gt; n_0$\" for convenience from here.</p>"},{"location":"Chap03/3.2/#32-2","title":"3.2-2","text":"<p>Explain why the statement, \"The running time of algorithm A is at least $O(n^2)$,\"if meaningless.</p> <p>$A = O(n^2)$ means $\\exist n_0,c:A \\leq cn^2$ for $n&gt;n_0$.</p> <p>$A$ is at least $O(n)$ means $A \\geq \\text{a function that }\\leq cn^2$</p> <p>You can say A can be any function or no function. Both are meaningless.</p>"},{"location":"Chap03/3.2/#32-3","title":"3.2-3","text":"<p>Is $2^{n+1}=O(2^n)$? Is $2^{2n}=O(2^n)$?</p> <ul> <li>$2^{n+1} \\leq 2*2^n$, Yes</li> <li>$2^{2n} \\leq c \\cdot 2^n \\text{ implies } c \\geq 2^n$, no constant satisfies it. No</li> </ul>"},{"location":"Chap03/3.2/#32-4","title":"3.2-4","text":"<p>Prove Theorem 3.1.</p> <p>$$ \\begin{aligned} &amp; f(n) = O(g(n)) \\quad and \\quad f(n)=\\Omega(g(n))\\cr &amp; \\implies c_1 g(n)\\leq f(n) \\leq c_2g(n)\\cr &amp; \\implies f(n) = \\Theta(g(n))\\cr &amp; f(n) = \\Theta(g(n))\\cr &amp; \\implies c_1 g(n)\\leq f(n) \\leq c_2g(n)\\cr &amp; \\implies f(n) = O(g(n)) \\quad and \\quad f(n)=\\Omega(g(n))\\cr \\end{aligned} $$</p> <p>We have $f(n) = \\Theta(g(n))$ if and only if $f(n) = \\Omega(g(n))$ and $f(n) = O(g(n))$</p>"},{"location":"Chap03/3.2/#32-5","title":"3.2-5","text":"<p>Prove that the running time of an algorithm is $\\Theta(g(n))$ if and only if its worst-case running time $O(g(n))$ and its best-casse running time is $\\Omega (g(n))$.</p> <p>$$ \\begin{aligned}     A=\\Theta(g(n)) &amp; \\implies c_1 g(n) \\leq A \\leq c_2 g(n) \\text{ for all cases}\\cr     &amp; \\implies \\text{worst-case running time}= O(n)\\cr     &amp; \\And \\text{best-case running time}=\\Omega(g(n))\\cr     \\text{worst-case running time}= O(n) &amp; \\implies A \\leq c_2 g(n) (1)\\cr     \\text{best-case running time}=\\Omega(g(n)) &amp; \\implies c_1 g(n) \\leq A (2)\\cr     (1) \\And (2) &amp; \\implies A=\\Theta(g(n))\\cr \\end{aligned} $$</p>"},{"location":"Chap03/3.2/#32-6","title":"3.2-6","text":"<p>Prove that $o(g(n)) \\cap \\omega (g(n))$ is empty set.</p> <p>$f(n)c(g(n)) \\implies NIL$"},{"location":"Chap03/3.2/#32-7","title":"3.2-7","text":"<p>We can extend our notation to the case of two parameters n and m that can go to $\\infty$ independently at different rates. For a given function $g(n,m)$, we denote by $O(g(n,m))$ the set of functions</p> <p>$$ \\begin{aligned} O(g(n, m)) =\\lbrace f(n, m):   &amp; \\text{ there exist positive constants } c, n_0, \\text{ and } m_0 \\cr   &amp; \\text{ such that } 0 \\leq f(n, m) \\leq cg(n, m)\\cr   &amp; \\text{ for all } n \\geq n_0 \\text{ or } m \\geq m_0.\\rbrace\\cr \\end{aligned} $$</p> <p>Give corresponding definitionss for $\\Omega(g(n,m))$ and $\\Theta(g(n,m))$.</p> <p>$$ \\begin{aligned} \\Theta(g(n, m)) =\\lbrace f(n, m): &amp; \\text{ there exist positive constants } c_1,c_2, n_0, \\text{ and } m_0 \\cr &amp; \\text{ such that } 0 \\leq c_1g(n,m)\\leq f(n, m) \\leq c_2g(n, m)\\cr &amp; \\text{ for all } n \\geq n_0 \\text{ or } m \\geq m_0.\\rbrace\\cr \\Omega(g(n, m)) =\\lbrace f(n, m): &amp; \\text{ there exist positive constants } c, n_0, \\text{ and } m_0 \\cr &amp; \\text{ such that } 0 \\leq cg(n,m)\\leq f(n, m)\\cr &amp; \\text{ for all } n \\geq n_0 \\text{ or } m \\geq m_0.\\rbrace\\cr \\end{aligned} $$</p>"},{"location":"Chap03/3.3/","title":"3.3 Standard notations and common functions","text":""},{"location":"Chap03/3.3/#33-1","title":"3.3-1","text":"<p>Show that if $f(n)$ and $g(n)$ are monotonically increasing functions, then so are the functions $f(n)+g(n)$ and $f(g(n))$, and if $f(n)$ and $g(n)$ are in addition nonnegative, then $f(n)\\cdot g(n)$</p> <p>$$ \\begin{aligned}     \\forall n_1 \\geq n_2: &amp; f(n_1) \\geq f(n_2) \\And g(n_1) \\geq g(n_2)\\cr     \\implies &amp; f(n_1)+g(n_1) \\geq f(n_2)+g(n_2)\\cr     \\implies &amp; f(g(n_1) \\geq g(n_2)) \\geq f(g(n_2))\\cr     &amp; if \\quad f(n) \\geq 0 \\And g(n) \\geq 0\\cr     \\implies &amp; f(g(n_1)\\geq g(n_2)\\geq 0)\\geq f(g(n_2)) \\end{aligned} $$</p>"},{"location":"Chap03/3.3/#33-2","title":"3.3-2","text":"<p>Prove that $\\lfloor \\alpha n \\rfloor +\\lceil (1-\\alpha )n \\rceil = n$ for any integer n and real number $\\alpha$ in the range $0 \\leq \\alpha \\leq 1$</p> <p>$$ \\begin{aligned}     \\lfloor \\alpha n \\rfloor +\\lceil (1-\\alpha )n \\rceil &amp; = \\lfloor \\alpha n \\rfloor +\\lceil -\\alpha n\\rceil + n\\cr     &amp;=\\lfloor \\alpha n \\rfloor - \\lfloor \\alpha n \\rfloor +n\\cr     &amp;=n \\end{aligned} $$</p>"},{"location":"Chap03/3.3/#33-3","title":"3.3-3","text":"<p>Use equation (3.14) or other means to show that $(n+o(n))^k = \\Theta(n^k)$ for any real constant k. Conclude that $\\lceil n \\rceil ^k$ and $\\lfloor n \\rfloor ^k = \\Theta(n^k)$.</p> <p>$$ \\begin{aligned}     n^k\\leq(n+o(n))^k=n^k(1+\\frac {o(n)}n)^k \\leq n^k e^{\\frac {o(n)}n} \\leq en^k\\cr     \\implies (n+o(n))^k = \\Theta(n^k)\\cr     \\lceil n \\rceil ^k= (n+o(n))^k=\\Theta(n^k)\\cr     \\lfloor n \\rfloor ^k = (n+o(n))^k=\\Theta(n^k)\\cr \\end{aligned} $$</p>"},{"location":"Chap03/3.3/#33-4","title":"3.3-4","text":"<p>Prove the following</p> <p>a. Equation(3.21).</p> <p>b. Equations(3.26)-(3.28).</p> <p>c. $\\lg(\\Theta(n))=\\Theta(\\lg n)$</p> <ul> <li>a. $a^{\\lg _b c} = a^{\\frac {lg _a c}{\\lg _a b}}=c^{\\lg _b a}$</li> <li>b.</li> </ul> <p>$$ \\begin{aligned}     n! = \\sqrt {2\\pi n}(\\frac n e)^n(1+\\Theta(\\frac{1}{n}))\\cr     \\forall c&gt;0,\\lim_{n \\to \\infin} \\frac {n!=\\frac{\\sqrt {2\\pi n}} {e^n}(1+\\Theta(\\frac 1 n))n^n} {cn^n} = \\lim_{n \\to \\infin} \\frac{\\sqrt {2\\pi n}} {e^n}(1+\\Theta(\\frac 1 n)) = 0\\cr     \\implies n!=o(n^n)\\text{ (Equation(3.21))}\\cr     \\lim_{n \\to \\infin} \\frac {n!=\\frac{\\sqrt {2\\pi n}} {e^n}(1+\\Theta(\\frac 1 n))n^n} {2^n} = \\lim_{n \\to \\infin }\\sqrt{2\\pi n}(1+\\Theta(\\frac 1 n))(\\frac{n}{2e})^n = \\infin \\cr     \\implies n! =\\omega(n) \\text{ (Equation(3.27))}\\cr     \\lg (n!)=n\\lg n+ \\frac{1}{2}\\lg(2\\pi n)+n\\lg e+\\lg(1+\\Theta(\\frac{1}{n})) = \\Theta(n\\lg n)\\text{ (Equation(3.28))}\\cr \\end{aligned} $$</p> <ul> <li>c.</li> </ul> <p>$$ \\begin{aligned}     \\forall f(n) \\in \\Theta(n)\\cr     c_3\\lg n\\leq \\lg {c_1 n} \\leq \\lg(f(n))\\leq \\lg{c_2 n} \\leq c_4\\lg n\\cr     \\implies \\lg(\\Theta(n))=\\Theta(\\lg n) \\end{aligned} $$</p>"},{"location":"Chap03/3.3/#33-5","title":"3.3-5","text":"<p>Is the function $\\lceil \\lg n \\rceil !$ polynomially bounded? Is the function $\\lceil \\lg \\lg n \\rceil !$ polynomially bounded?</p> <p>Polynomially bounded:$f(n) \\leq cn^k \\implies \\lg(f(n))=ck\\lg(n)\\implies \\lg(f(n))=O(\\lg n)$</p> <p>$$ \\begin{aligned}     \\lg(\\lceil \\lg n\\rceil !)&amp; =\\Theta(\\lceil \\lg n\\rceil \\lg(\\lceil \\lg n\\rceil))\\cr     &amp;=\\Theta(\\lg n \\lg(\\lg n))\\cr     &amp;\\implies \\lceil \\lg n\\rceil !\\text{ is not polynomially bouded}\\cr     \\lg \\lceil \\lg \\lg n \\rceil !&amp;=\\Theta(\\lceil \\lg \\lg n\\rceil \\lg\\lceil\\lg\\lg n\\rceil)\\cr     &amp; =O(\\lg^2\\lg n)\\cr     &amp; =O(\\lg n)\\cr     &amp; \\implies\\lceil\\lg\\lg n\\rceil \\text{ is Polynomially bounded} \\end{aligned} $$</p>"},{"location":"Chap03/3.3/#33-6","title":"3.3-6","text":"<p>Which is asymptotically larger: $\\lg(\\lg^{\\ast}n)$ or $\\lg^{\\ast}(\\lg n)$?</p> <p>$$ \\begin{aligned}     n=2^k\\cr     \\lim_{n\\to\\infin}\\frac{\\lg(\\lg^{\\ast}n)}{\\lg^{\\ast}(\\lg n)}&amp; = \\lim_{k\\to\\infin}\\frac{\\lg(\\lg^{\\ast} 2^k)}{\\lg^{\\ast}(\\lg 2^k)}\\cr     &amp;= \\lim_{k\\to\\infin} \\frac{\\lg(1+\\lg^{\\ast}k)}{\\lg^{\\ast}k}\\cr     &amp; =\\lim_{t\\to \\infin}\\frac{\\lg(1+t)}{t}\\cr     &amp; = 0 \\end{aligned} $$ $\\lg^{\\ast}(\\lg n)$ is larger.</p>"},{"location":"Chap03/3.3/#33-7","title":"3.3-7","text":"<p>Show that the golden ratio $\\phi$ and its conjugate $\\hat{\\phi}$ both satisfy the equation $x^2 = x + 1$</p> <p>$$ \\begin{aligned}     ax^2+bx+c=0\\implies x=\\frac{-b\\pm\\sqrt{b^2-4ac}}{2a}\\cr     x^2-x-1=0 \\implies x=\\frac{1 \\pm\\sqrt{5}}{2}=\\phi \\And \\hat\\phi \\end{aligned} $$</p>"},{"location":"Chap03/3.3/#33-8","title":"3.3-8","text":"<p>prove by induction that the i-th Fibonacci num satisfies the equation $$ F_i = (\\phi^i - \\hat{\\phi}^i)/\\sqrt{5} $$ where $\\phi$ is the golden ratio and $\\hat{\\phi}$ is its conjugate.</p> <p>For inductive case:</p> <p>$$ \\begin{aligned}     &amp; \\text{truth: }\\phi\\hat\\phi=-1,\\phi+\\hat\\phi=1\\cr     \\text{assume }:\\cr     F_{i-1} &amp;= (\\phi^{i-1} - \\hat{\\phi}^{i-1})/\\sqrt{5}\\cr     F_{i} &amp;= (\\phi^{i} - \\hat{\\phi}^{i})/\\sqrt{5}\\cr     \\text{then:}\\cr     F_{i-1}+F_i &amp; =(\\phi^{i-1} - \\hat{\\phi}^{i-1})/\\sqrt{5}+(\\phi+\\hat\\phi)(\\phi^i - \\hat{\\phi}^i)/\\sqrt{5}\\cr     &amp; = (\\phi^{i+1}-\\hat\\phi^{i+1})/\\sqrt 5+((1+\\phi\\hat\\phi)\\phi^{i-1}-(1+\\phi\\hat\\phi)\\hat\\phi^{i-1})/\\sqrt 5\\cr     &amp; =F_{i+1}\\cr \\end{aligned} $$</p> <p>For base case: $$ \\begin{aligned}     (\\phi^{1} - \\hat{\\phi}^{1})/\\sqrt{5}=1=F_1\\cr     (\\phi^{2} - \\hat{\\phi}^{2})/\\sqrt{5}=(\\phi^{1}+1 - (\\hat{\\phi}^{1}+1))/\\sqrt{5}=1=F_2\\cr \\end{aligned} $$</p> <p>Tips: I will omit details like\"assume $F_i$ $F_{i-1}$\" and proof of base case from now on for convenience.</p>"},{"location":"Chap03/3.3/#33-9","title":"3.3-9","text":"<p>Show that $k\\lg k = \\Theta(n)$ implies $k = \\Theta(n/\\lg n)$.</p> <p>$$ \\begin{aligned}     &amp; k\\lg k=\\Theta(n)\\cr     \\implies &amp; n=\\Theta(k\\lg k)\\cr     \\implies &amp; \\lg n = \\Theta(\\lg k + \\lg\\lg k)=\\Theta(\\lg k)\\cr     \\implies &amp; n/\\lg n = \\Theta(k\\lg k)/\\Theta(\\lg k)=\\Theta (k)\\cr     \\implies &amp; k = \\Theta(n/\\lg n) \\end{aligned} $$</p>"},{"location":"Chap03/Problems/3-1/","title":"3-1 Asymptotic behavior of polynomials","text":"<p>Let</p> <p>$$ P(n) = \\sum_{i=0}^d a_i n^i, $$</p> <p>where $a_d &gt; 0$, be a degree-d polynomial in n, and let k be a constant. Use the definitions of the asymptotic notations to prove the following properties.</p> <p>a. if $k \\geq d$, then $p(n)=O(n^k)$</p> <p>b. if $k \\leq d$, then $p(n)=\\Omega(n^k)$</p> <p>c. if $k =d$, then $p(n)=\\Theta(n^k)$</p> <p>d. if $k &gt; d$, then $p(n)=o(n^k)$</p> <p>e. if $k &lt; d$, then $p(n)=\\omega(n^k)$</p>"},{"location":"Chap03/Problems/3-1/#a","title":"a","text":"<p>To prove $p(n)=O(n^k)$, we only need to prove:</p> <p>$$ \\begin{aligned}     \\sum_{i=0}^{d}a_in^i \\leq cn^k (\\text{ for }n \\geq n_0)\\cr \\end{aligned} $$</p> <p>Which is implied by</p> <p>$$ \\begin{aligned}     c \\geq \\sum_{i=0}^da_in^{i-k}  (\\text{ for }n \\geq n_0)\\cr \\end{aligned} $$ since $\\sum_{i=0}^{d}a_i \\geq \\sum_{i=0}^da_in^{i-k} (\\text{ for }n \\geq 1)$</p> <p>$\\text{choose } c = \\sum_{i=0}^{d}a_i$ and $n_0 = 1$ satisfies.</p> <ul> <li>Tips: the description of $n&gt;n_0$ and $c$ will be omitted next time.</li> </ul>"},{"location":"Chap03/Problems/3-1/#b","title":"b","text":"<p>We only need to prove:</p> <p>$$     \\sum_{i=0}^{d}a_in^i \\geq cn^k $$</p> <p>which is implied by:</p> <p>$$ \\begin{aligned}     c \\leq \\sum_{i=0}^da_in^{i-k}\\cr     c=a_d \\leq \\sum_{i=0}^da_in^{i-k}\\cr     n_0=1 \\end{aligned} $$</p>"},{"location":"Chap03/Problems/3-1/#c","title":"c","text":"<p>We only need to prove:</p> <p>$$     c_1n^d\\leq \\sum_{i=0}^{d}a_in^i \\leq c_2n^{d} $$</p> <p>which is implied by:</p> <p>$$ \\begin{aligned}     c_1 \\leq \\sum_{i=0}^da_in^{i-d} \\leq c_2\\cr     c_1=a_d \\leq \\sum_{i=0}^da_in^{i-d} \\leq \\sum_{i=0}^{d} a_i=c_2\\cr     n_0=1 \\end{aligned} $$</p>"},{"location":"Chap03/Problems/3-1/#d","title":"d","text":"<p>We only need to prove: $$ \\begin{aligned}     \\forall c &gt;0,\\exist n_0 \\text{ satisfies: }\\cr     \\sum_{i=0}^{d} a_i n^i &lt; cn^k (\\text{ for } n &gt; n_0)\\cr     \\impliedby c &gt; \\sum_{i=0}^{d} a_in^{i-k}\\cr     \\text{since }\\sum_{i=0}^{d} a_in^{d-k} \\geq \\sum_{i=0}^{d} a_in^{i-k}\\cr     n_0=\\sqrt[d-k]{\\frac {c} {\\sum_{i=0}^d a_i}} \\end{aligned} $$</p>"},{"location":"Chap03/Problems/3-1/#e","title":"e","text":"<p>We only need to prove: $$ \\begin{aligned}     \\forall c &gt;0,\\exist n_0 \\text{ satisfies: }\\cr     \\sum_{i=0}^{d} a_in^i &gt; cn^k (\\text{ for }n&gt;n_0)\\cr     \\impliedby c &lt; \\sum_{i=0}^{d} a_in^{i-k}\\cr     \\text{since } a_d n^{d-k} \\leq \\sum_{i=0}^{d} a_in^{i-k}\\cr     n_0=\\sqrt[d-k]{\\frac {c} {a_d}} \\end{aligned} $$</p>"},{"location":"Chap03/Problems/3-2/","title":"3-2 Relative asymptotic growths","text":"<p>Indicate, for each pair of expressions $(A, B)$ in the table below whether A is $O, o,\\Omega,\\omega$, or $\\Theta$ of B. Assume that $k \\geq 1,\\epsilon &gt;,$ and $c&gt;1$ are constants. Write your answer in the foem of the table with \"yes\" or \"no\" written in each box.</p> A B O o $\\Omega$ $\\omega$ $\\Theta$ $\\lg^k n$ $n^{\\epsilon}$ yes yes no no no $n^k$ $c^n$ yes yes no no no $\\sqrt{n}$ $c^{\\sin n}$ no no no no no $2^n$ $2^{n/2}$ no no yes yes no $n^{\\lg c}$ $c^{\\lg n}$ yes no yes no yes $\\lg (n!)$ $\\lg (n^n)$ yes no yes no yes"},{"location":"Chap03/Problems/3-3/","title":"3-3 Ordering by asymptotic growth rates","text":"<p>a. Rank the following fuctions vy oefer of growth. That is, find an arrangement $g_1,g_2,\\dots,g_{30}$ of the functions satisfying $g_1=\\Omega(g_2),g_2=\\Omega(g_3),\\dots ,g_{29}=\\Omega(g_{30})$. Partion your list into equivalence classes such that functions $f(n)$ and $g(n)$ belong to the same class if and only if $f(n)$ = $\\Theta g(n)$</p> <p>$$ \\begin{array}{} &amp; \\lg(\\lg^{\\ast}n) &amp; 2^{\\lg^{\\ast}n} &amp; (\\sqrt {2})^{\\lg n} &amp; n^2 &amp; n! &amp; (\\lg n)!\\cr &amp; (3/2)^n &amp; n^3 &amp; \\lg^2n &amp; \\lg(n!) &amp; 2^{2^n} &amp; n^{1/\\lg n}\\cr &amp; \\ln\\ln n &amp; \\lg^{\\ast}n &amp; n\\cdot 2^n &amp; n^{\\lg\\lg n} &amp; \\ln n &amp; 1\\cr &amp; 2^{\\lg n} &amp; (\\lg n)^{\\lg n} &amp; e^n &amp; 4^{\\lg n} &amp; (n+1)! &amp; \\sqrt{\\lg n}\\cr &amp; \\lg^{\\ast}(\\lg n) &amp; 2^{\\sqrt{2 \\lg n}}&amp; n &amp; 2^n&amp; n\\lg n  &amp; 2^{2^{n+1}}\\cr \\end{array} $$</p> <p>b. Give an example of a single nonnegative function $f(n)$ such that for all functions $g_i(n)$ in part (a), $f(n)$ is neither $O(g_i(n))$ nor $\\Omega (g_i(n))$.</p>"},{"location":"Chap03/Problems/3-3/#a","title":"a","text":"<p>$$ \\begin{aligned}     2^{2^{n+1}}\\cr     2^{2^n}\\cr     (n+1)!\\cr     n!\\cr     e^n\\cr     n\\cdot 2^n\\cr     2^n\\cr     (3/2)^n\\cr     n^{\\lg\\lg n} \\And (\\lg n)^{\\lg n}\\cr     (\\lg n)!\\cr     n^3\\cr     n^2 \\And 4^{\\lg n} \\cr     \\lg(n!) \\And n\\lg n\\cr     2^{\\lg n} \\And n\\cr     (\\sqrt {2})^{\\lg n}\\cr     2^{\\sqrt{2\\lg n}}\\cr     \\lg^2 n\\cr     \\ln n\\cr     \\sqrt{\\lg n}\\cr     \\ln\\ln n\\cr     2^{\\lg^{\\ast}n}\\cr     \\lg^{\\ast}n \\And \\lg^{\\ast}(\\lg n)\\cr     \\lg(\\lg^{\\ast}n)  \\cr     n^{1/\\lg n}\\And 1\\cr \\end{aligned} $$</p>"},{"location":"Chap03/Problems/3-3/#b","title":"b","text":"<p>$|\\sin{n}|\\cdot 2^{2^{2^{n+1}}}$</p>"},{"location":"Chap03/Problems/3-4/","title":"3-4 Asymptotic notation properties","text":"<p>Let $f(n)$ and $g(n)$ be asymptotically positive functions. Prove or disprove each of the following conjectures.</p> <p>a. $f(n)=O(g(n))$ implies $g(n)=O(f(n))$.</p> <p>b. $f(n)+g(n)=\\Theta(\\min(f(n),g(n)))$.</p> <p>c. $f(n)=O(g(n))$ implies $\\lg(f(n))=O(\\lg(g(n)))$, where $\\lg g(n)\\geq 1$ and $f(n)\\geq 1$ for all sufficiently large $n$.</p> <p>d. $f(n)=O(g(n))$ implies $2^{f(n)} = O(2^{g(n)})$.</p> <p>e. $f(n)=O((f(n))^2)$.</p> <p>f. $f(n)=O(g(n))$ implies $g(n)=\\Omega(f(n))$.</p> <p>g. $f(n)=\\Theta(f(n/2))$.</p> <p>h. $f(n)+o(f(n))=\\Theta(f(n))$. </p>"},{"location":"Chap03/Problems/3-4/#a","title":"a","text":"<p>Disprove: $n=O(n^2)$ but $n^2\\neq O(n)$</p>"},{"location":"Chap03/Problems/3-4/#b","title":"b","text":"<p>Disprove:$n+n^2\\neq \\Theta(\\min(n,n^2))$</p>"},{"location":"Chap03/Problems/3-4/#c","title":"c","text":"<p>prove:</p> <p>$$ \\begin{aligned}     0\\leq f(n)\\leq cg(n) &amp; \\implies \\lg(f(n))\\leq \\lg c+\\lg(g(n))\\cr     \\text{to prove: }&amp;\\lg(f(n)) \\leq d\\lg(g(n))\\cr     \\text{only need to prove: }&amp; \\lg c + \\lg(g(n)) \\leq d\\lg(g(n))\\cr     \\text{choose } &amp; \\quad d = 1+\\lg c \\text{ satisfies} \\end{aligned} $$</p>"},{"location":"Chap03/Problems/3-4/#d","title":"d","text":"<p>prove:</p> <p>$$ \\begin{aligned}     0\\leq f(n)\\leq cg(n) \\implies 2^{f(n)}\\leq 2^{cg(n)}=2^c\\cdot c^{g(n) } \\end{aligned} $$</p>"},{"location":"Chap03/Problems/3-4/#e","title":"e","text":"<p>disprove: $\\frac{1}{n}\\neq O(\\frac{1}{n^2})$</p>"},{"location":"Chap03/Problems/3-4/#f","title":"f","text":"<p>prove: $0\\leq f(n)\\leq cg(n) \\implies g(n)=\\Omega (f(n))$</p>"},{"location":"Chap03/Problems/3-4/#g","title":"g","text":"<p>disprove:$2^n\\neq\\Theta(2^{\\frac{n}{2}})$</p>"},{"location":"Chap03/Problems/3-4/#h","title":"h","text":"<p>prove: $f(n)\\leq f(n)+o(f(n))\\leq 2f(n)$ since $0\\leq o(f(n))&lt; f(n)$</p>"},{"location":"Chap03/Problems/3-5/","title":"3-5 Manipulating asymptotic notation","text":"<p>Let $f(n)$ and $g(n)$ be asymptotically positive functions. Prove the following identities: a. $\\Theta(\\Theta(f(n)))=\\Theta(f(n))$.</p> <p>b. $\\Theta(f(n))+O(f(n))=\\Theta(f(n))$.</p> <p>c. $\\Theta(f(n))+\\Theta(g(n)) =\\Theta(f(n)+g(n))$.</p> <p>d. $\\Theta(f(n))\\cdot \\Theta(g(n))=\\Theta(f(n)\\cdot g(n))$.</p> <p>e. Argue that for any real constants $a_1,b_1&gt;0$ and integer constants $k_1,k_2$, the following asymptotic bound holds:</p> <p>$(a_1 n)^{k_1}\\lg^{k_2}(a_2 n)=\\Theta(n^{k_1}\\lg^{k_2}n)$.</p> <p>$\\star$ f. Prove that for $S\\subseteq \\Z$, we have</p> <p>$$ \\sum_{k\\in S}\\Theta(f(k))=\\Theta(\\sum_{k\\in S}f(k)), $$</p> <p>assuming that both sums converge.</p> <p>$\\star$ g. Show that for $S \\subseteq \\Z$, the following asymptotic bound does not necessarily hold, even assuming that both products converge, by giving a counterexample:</p> <p>$$ \\prod_{k\\in S}\\Theta(f(k))=\\Theta(\\prod_{k\\in S}f(k)). $$ </p>"},{"location":"Chap03/Problems/3-5/#a","title":"a","text":"<p>$$ \\begin{aligned}     g(n)=\\Theta(f(n))\\cr     \\implies c_1 f(n)\\leq g(n) \\leq c_2 f(n)\\cr     h(n)=\\Theta(g(n))\\cr     \\implies c_1 d_1 f(n)\\leq d_1 g(n)\\leq h(n)\\leq d_2g(n)\\leq c_2 d_2 f(n)\\cr     \\implies h(n)=\\Theta(f(n))\\cr     \\implies\\Theta(\\Theta(f(n)))=\\Theta(f(n))\\cr \\end{aligned} $$</p>"},{"location":"Chap03/Problems/3-5/#b","title":"b","text":"<p>$$ \\begin{aligned}     \\forall g(n)=\\Theta(f(n)) \\And h(n)=\\Theta(f(n))\\cr     \\implies c_2 f(n) \\leq g(n)\\leq c_1 f(n) \\And 0 \\leq h(n)\\leq      df(n)\\cr      \\implies c_1f(n)\\leq g(n)+h(n)\\leq (c_1+d)f(n)\\cr      \\implies g(n)+h(n)=\\Theta(f(n))\\cr      \\implies \\Theta(f(n))+O(f(n))=\\Theta(f(n))\\cr \\end{aligned} $$</p>"},{"location":"Chap03/Problems/3-5/#c","title":"c","text":"<p>$$ \\begin{aligned}     \\forall A(n)=\\Theta(f(n))\\And B(n)=\\Theta(g(n))\\cr     \\implies c_1 f(n)\\leq A(n)\\leq c_2 f(n)\\And d_1 f(n)\\leq B(n) \\leq d_2 f(n)\\cr     \\implies \\min(c_1,d_1)(f(n)+g(n))\\leq A(n)+B(n)\\leq \\max(c_2+d_2)(f(n)+g(n))\\cr     \\implies \\Theta(f(n))+\\Theta(g(n)) =\\Theta(f(n)+g(n)) \\end{aligned} $$</p>"},{"location":"Chap03/Problems/3-5/#d","title":"d","text":"<p>$$ \\begin{aligned}     \\forall A(n)=\\Theta(f(n))\\And B(n)=\\Theta(g(n))\\cr     \\implies c_1 f(n)\\leq A(n)\\leq c_2 f(n)\\And d_1 f(n)\\leq B(n) \\leq d_2 f(n)\\cr     \\implies c_1 d_1f(n)\\cdot g(n)\\leq A(n)\\cdot B(n)\\leq c_2 d_2f(n)\\cdot g(n)\\cr     \\implies \\Theta(f(n))\\cdot \\Theta(g(n)) =\\Theta(f(n)\\cdot g(n)) \\end{aligned} $$</p>"},{"location":"Chap03/Problems/3-5/#e","title":"e","text":"<p>$$ \\begin{aligned}     \\text{we only need to prove: }c_1 n^{k_1}\\lg^{k_2}n\\leq (a_1 n)^{k_1}\\lg^{k_2}(a_2 n)\\leq c_2 n^{k_1}\\lg^{k_2}n\\cr     \\text{which is implied by: }d_1 lg^{k_2}n\\leq\\lg^{k_2}(a_2 n)\\leq d_2\\lg^{k_2}n\\cr     \\text{which is implied by: }e_1\\lg n\\leq \\lg a+\\lg n \\leq e_2 \\lg n\\cr     \\text{which is implied by: }e_1\\leq \\lg a/\\lg n +1 \\leq e_2\\cr     \\text{choose }e_1=1/2 \\quad e_2=2 \\quad n_0=\\lg max(a^2,2^{\\lg a}) \\text{ satisfies}\\cr \\end{aligned} $$</p>"},{"location":"Chap03/Problems/3-5/#star-f","title":"$\\star$ f","text":"<p>$$ \\begin{aligned}     g(k)=\\Theta(f(k))\\cr     \\implies \\sum_{k\\in S} c_kf(k)\\leq \\sum_{k\\in S} g(k)\\leq \\sum_{k\\in S} d_kf(k)\\cr     \\implies \\min( c_k) \\sum_{k\\in S} f(k)\\leq \\sum_{k\\in S} g(k)\\leq \\max(d_k)\\sum_{k\\in S} f(k)\\cr     \\implies \\sum_{k\\in S}\\Theta(f(k))=\\Theta(\\sum_{k\\in S}f(k))\\cr \\end{aligned} $$</p>"},{"location":"Chap03/Problems/3-5/#star-g","title":"$\\star$ g","text":"<p>$\\prod_{k\\in S} \\frac 1 2=\\prod_{k\\in S} \\Theta(\\frac{1}{2^n})=\\Theta(0)\\neq \\Theta(\\prod_{k\\in S} 1)=\\Theta(1)$</p>"},{"location":"Chap03/Problems/3-6/","title":"3-6 Variations on $O$ and $\\Omega$","text":"<p>Some authors define $\\Omega$-notation in a slightly different way than this textbook does. We'll use the nomenclature $\\stackrel{\\infty}{\\Omega}$ (read \"omega infinity\") for this alternative definition. We say that f(n) = $\\stackrel{\\infty}{\\Omega}(g(n))$if there exists a positive constant $c$ such that $f(n) \\geq cg(n) \\geq 0$ for infinitely many integers n. a. Show that for any two asymptotically nonnegative functions $f(n)$ and $g(n)$, we have $f(n)=O(g(n))$ or $f(n)=\\stackrel{\\infty}{\\Omega}(g(n))$ (or both).</p> <p>b. Show that there exist two asymptotically nonnegative functions $f(n)$ and $g(n)$ for which neither $f(n) = O(g(n))$ nor $f(n) = \\Omega(g(n))$ holds.</p> <p>c. Describe the potential advantages and disadvantages of using $\\stackrel{\\infty}{\\Omega}$-notation instead of $\\Omega$-notation to characterize the running times of programs.</p> <p>d. Some authors also define $O$ in a slightly different manner. We\u2019ll use $O'$ for the alternative definition: $f(n)=O'(g(n))$ if and only if $|f(n)|=O(g(n))$.</p> <p>What happens to each direction of the \"if and only if\" in Theorem 3.1 on page 56 if we substitute $O'$ for $O$ but still use $\\Omega$?</p> <p>e Some authors define $\\tilde{O}$(read \"soft-oh\") to mean O with logarithmic factors ignored:</p> <p>$$ \\begin{aligned} \\tilde{O}(g(n))=\\lbrace f(n) : &amp; \\text{ there exist positive constants }c, k, \\text{ and } n_0 \\text{ such that }\\cr &amp; 0\\leq f(n)\\leq cg(n)\\lg^k(n) \\text{ for all } n\\geq n_0\\rbrace .\\cr \\end{aligned} $$</p> <p>Define $\\tilde{\\Omega}$ and $\\tilde{\\Theta}$ in a similar manner. Prove the corresponding analog to Theorem 3.1.</p>"},{"location":"Chap03/Problems/3-6/#a","title":"a","text":"<p>$$ \\begin{aligned}     \\text{if } f(n)\\neq O(g(n))\\cr     \\text{there must be infinite integers that }f(n) \\geq cg(n)\\geq 0\\cr     \\text{otherwise choose the last } n = \\text{ that } f(n)\\geq cg(n) \\text{ as } n_0\\cr     \\text{for all }n\\geq n_0,f(n)\\leq cg(n)\\implies f(n)=O(g(n))\\cr \\end{aligned} $$</p>"},{"location":"Chap03/Problems/3-6/#b","title":"b","text":"<p>$f(n)=n|\\sin n|,g(n)=1$</p>"},{"location":"Chap03/Problems/3-6/#c","title":"c","text":"<ul> <li>advantages: By a we can know that $O$ and $\\stackrel{\\infty}{\\Omega}$ can characterize relationship between any two function.</li> <li>disadvantages: Not precise.</li> </ul>"},{"location":"Chap03/Problems/3-6/#d","title":"d","text":"<p>$$ \\begin{aligned}     \\forall f(n) , g(n):\\cr     f(n)=\\Theta(g(n)) \\implies f(n)=\\Omega(g(n)) \\And f(n)=O'(g(n))\\cr     \\text{but the conversion is not true} \\end{aligned} $$</p>"},{"location":"Chap03/Problems/3-6/#e","title":"e","text":"<p>$$ \\begin{aligned}     \\tilde\\Omega(g(n))=\\lbrace f(n): &amp; \\text{ there exist positive constants }c,k,\\text{ and } n_0 \\text{ such that}\\cr     &amp; 0\\leq f(n) \\leq cg(n)\\lg^{k}(n) \\text{ for all }n \\geq n_0\\rbrace .\\cr     \\tilde\\Theta(g(n))=\\lbrace f(n): &amp; \\text{ there exist positive constants }c_1,c_2,k,\\text{ and } n_0 \\text{ such that}\\cr     &amp; 0\\leq c_1g(n)\\lg^{k}(n)\\leq f(n) \\leq c_2g(n)\\lg^{k}(n) \\text{ for all }n \\geq n_0\\rbrace .\\cr     \\text{according to there definity: } &amp; f(n)=\\tilde\\Theta(g(n))\\iff f(n)=\\tilde O(g(n)) \\And f(n)=\\tilde\\Omega (g(n))\\cr \\end{aligned} $$</p>"},{"location":"Chap03/Problems/3-7/","title":"3-7 Iterated functions","text":"<p>We can apply the iteration operator * used in the $\\lg^{\\ast}$ function to any monotonically increasing function $f(n)$ over the real. For a given constant $c \\in \\R$, we define the iterated dunction $f_{c}^{\\ast}$ by</p> <p>$f_{c}^{\\ast}=\\min \\lbrace i\\geq0:f^{(i)}\\leq c\\rbrace$,</p> <p>which need not be well defined in all cases. In other words, the quantity $f_{c}^{\\ast}(n)$ is the minimum number of iterated applications of the function $f$ required to reduce its argument down to $c$ or less.</p> <p>For each of the functions $f(n)$ and constants $c$ in the table below, give as tight a bound as possible on $f_{c}^{\\ast}(n)$. If there is no $i$ such that $f^{(i)} \\leq c$, write \"undefined\" as your answer.</p> $f(n)$ $c$ $f_{c}^{\\ast}(n)$ $n-1$ $0$ $\\Theta(n)$ $\\lg n$ $1$ $\\Theta(\\lg^{\\ast}n)$ $n/2$ $1$ $\\Theta(\\lg n)$ $n/2$ $2$ $\\Theta(\\lg n)$ $\\sqrt{n}$ $2$ $\\theta(\\lg\\lg n)$ $\\sqrt{n}$ $1$ undefined $n^{1/3}$ $2$ $\\Theta(\\log_3\\lg n)$"},{"location":"Chap04/4.1/","title":"4.1 Multiplying square matrices","text":"<p>Note: You may wish to read Section 4.5 before attempting some of these exercises.</p>"},{"location":"Chap04/4.1/#41-1","title":"4.1-1","text":"<p>Generalize MATRIX-MULTIPLY-RECURSIVE to multiple $n\\times n$ matrices for which $n$ is not necessarily an exact power of 2. Give a recurrence describing its running time. Argue that it runs in $\\Theta(n^3)$ time in the worst case.</p> <pre><code>MATRIX_MULTIPLY_RECURSIVE_GENERALIZE(A,B,C,n)\n    if n is not exact power of 2\n        new_n = next power of 2 of n//O(n)\n    use 0 to extend A B C into new_n*new_n matrix//O(n^2)\n        MATRIX_MULTIPLY_RECURSIVE(A,B,C,new_n)//Theta(n^3)\n    choose the n*n part of C as result by index method//O(1)\n    return \n</code></pre> <p>running time = $\\Theta(n^3)+O(n^2)+O(1)+O(n)=\\Theta(n^3)$</p>"},{"location":"Chap04/4.1/#41-2","title":"4.1-2","text":"<p>How quickly can you multiply a $kn\\times n$ matrix (kn rows and n columns) by an $n\\times kn$ matrix, where $k\\geq 1$ , using MATRIX-MULTIPLY-RECURSIVE as a subtoutine? Answer the same question for multiplying an $n\\times kn$ matrix by a $kn\\times n$ matrix. Which is asymptotically faster, and by how much?</p> <ul> <li>mulyiplying a $kn\\times n$ matrix by an $n\\times kn$ matrix needs $k^2$ times' call of MATRIX-MULTIPLY-RECURSIVE(A,B,C,n), taking $\\Theta(k^2n^3)$ in total.</li> <li>mulyiple a $n\\times kn$ matrix by an $kn\\times n$ matrix needs $k$ times' call of MATRIX-MULTIPLY-RECURSIVE(A,B,C,n), and need $k-1$ times' addition, taking $\\Theta(kn^3+kn^2)=\\Theta(kn^3)$ in total.</li> <li>The latter is asympototicallt faster.</li> </ul>"},{"location":"Chap04/4.1/#41-3","title":"4.1-3","text":"<p>Suppose that instead of partitioning matrices by index calculation in MATRIX-MULTIPLY-RECURSIVE, you copy the appropriate elements of A,B,and C into separate $n/2\\times n/2$ submarices $A_{11},A_{12},A_{21},A_{22};B_{11},B_{12},B_{21},B_{22}$; and $C_{11},C_{12},C_{21},C_{22}$, respectively. After the recursive calls, you copy the results from $C_{11},C_{12},C_{21}$, and $C_{22}$ back into the appropriate places in $C$. How does recurrence (4.9) change, and what is its solution?</p> <p>$T(n)=8T(n/2)+\\Theta(n^2)$</p> <p>According to the master theorem, $T(n)=\\Theta(n^3)$</p>"},{"location":"Chap04/4.1/#41-4","title":"4.1-4","text":"<p>Write pseudocode for a divide-and-conquer algorithm MATRIX-ADD-RECURSIVE that sums two $n\\times n$ matrices $A$ and $B$ by partitioning each of them into four $n/2\\times n/2$ submatrices and then recursively summing corresponding pairs of submatrices. Assume that matrix partitioning uses $\\Theta(1)$-time index calculations. Write a recurrence for the worst-case running time of MATRIX-ADD-RECURSIVEE, and solve your recurrence. What happens if you use $\\Theta(n^2)$-time copying to implement the partitioning instead of index calculations?</p> <pre><code>MATRIX_ADD_RECURSIVEE(A,B,C,n)\n    if n==1\n        c11=a11+b11\n    partition A,B,C into n/2*n/2 submatrixs\n        A11,A12,A21,A22;B11,B12,B21,B22;and C11,C12,C21,C22; \n    MATRIX_ADD_RECURSIVEE(A11,B11,C11,n/2)\n    MATRIX_ADD_RECURSIVEE(A12,B12,C12,n/2)\n    MATRIX_ADD_RECURSIVEE(A21,B21,C21,n/2)\n    MATRIX_ADD_RECURSIVEE(A22,B22,C22,n/2)\n</code></pre> <p>$T(n)=4T(n/2)+\\Theta(1)$</p> <p>According to master theorem, $T(n)=\\Theta(n^2)$</p> <p>if use copying,$T(n)=4T(n/2)+\\Theta(n^2)\\implies T(n)=\\Theta(n^2\\lg n)$</p>"},{"location":"Chap04/4.2/","title":"4.2 Strassen\u2019s algorithm for matrix multiplication","text":"<p>Note: You may wish to read Section 4.5 before attempting some of these exercises.</p>"},{"location":"Chap04/4.2/#42-1","title":"4.2-1","text":"<p>Use Strassen\u2019s algorithm to compute the matrix product</p> <p>$$ \\begin{pmatrix}   1 &amp; 3\\cr   7 &amp; 5 \\end{pmatrix} \\begin{pmatrix}   6 &amp; 8\\cr   4 &amp; 2 \\end{pmatrix} $$</p> <p>Show your work.</p> <p>$$ \\begin{aligned}     S_1=B_{12}-B_{22}=8-2=6\\cr     S_2=A_{11}+A_{12}=1+3=4\\cr     S_3=A_{21}+A_{22}=7+5=12\\cr     S_4=B_{21}-B_{11}=4-6=-2\\cr     S_5=A_{11}+A_{22}=1+5=6\\cr     S_6=B_{11}+B_{22}=6+2=8\\cr     S_7=A_{12}-A_{22}=3-5=-2\\cr     S_8=B_{21}+B_{22}=4+2=6\\cr     S_9=A_{11}-A_{21}=1-7=-6\\cr     S_{10}=B_{11}+B_{12}=6+8=14\\cr     P_1=A_{11}\\cdot S_1=16=6\\cr     P_2=S_2\\cdot B_{22}=42=8\\cr     P_3=S_3\\cdot B_{11}=126=72\\cr     P_4=A_{22}\\cdot S_{4}=5(-2)=-10\\cr     P_5=S_5\\cdot S_6=68=48\\cr     P_6=S_7\\cdot S_8=-26=-12\\cr     P_7=S_9\\cdot S_10=-6*14=-84\\cr     C_{11}=C_{11}+P_5+P_4-P_2+P_6=48+(-10)-8+(-12)=18\\cr     C_{12}=C_{12}+P_1+P_2=6+8=14\\cr     C_{21}=C_{21}+P_3+P_4=72+(-10)=62\\cr     C_{22}=C_{22}+P_5+P_1-P_3-P_7=48+6-72-(-84)=66\\cr \\begin{pmatrix}     18 &amp; 14\\cr     62 &amp; 66\\cr \\end{pmatrix} \\end{aligned} $$</p>"},{"location":"Chap04/4.2/#42-2","title":"4.2-2","text":"<p>Write pseudocode for Strassen\u2019s algorithm.</p> <pre><code>Strassen(A,B,C,n)\n    if n ==1\n        c11=c11+a11*b11\n    partition A,B,C into (n/2)*(n/2) submatrixs\n    A11,A12,A21,A22;B11,B12,B21,B22;C11,C12,C21,C22\n    //get S O(n^2)\n    S1=B12-B22\n    S2=A11+A12\n    S3=A21+A22\n    S4=B21-B11\n    S5=A11+A22\n    S6=B11+B22\n    S7=A12-A22\n    S8=B21+B22\n    S9=A11-A21\n    S10=B11+B12\n    //creat P O(n^2)\n    creat P1...7\n    //recursion\n    Strassen(A11,S1,P1,n/2)\n    Strassen(S2,B22,P2,n/2)\n    Strassen(S3,B11,P3,n/2)\n    Strassen(A22,S4,P4,n/2)\n    Strassen(S5,S6,P5,n/2)\n    Strassen(S7,S8,P6,n/2)\n    Strassen(S9,S10,P7,n/2)\n    //conquer O(n^2)\n    C11=C11+P5+P4-P2+P6\n    C12=C12+P1+P2\n    C21=C21+P3+P4\n    C22=C22+P5+P1-P3-P7\n</code></pre>"},{"location":"Chap04/4.2/#42-3","title":"4.2-3","text":"<p>What is the largest $k$ such that if you can multiply $3\\times 3$ matrices using $k$ multiplications (not assuming commutativity of multiplication), then you can multiply $n\\times n$ matrices in $o(n^{\\lg 7})$ time? What is the running time of this algorithm?</p> <p>$$ \\begin{aligned}     T(n)=kT(n/3)+O(1)=o(n^{\\lg 7})\\cr     \\implies T(n)=\\Theta(n^{\\log_{3} k})=o(n^{\\lg 7})\\cr     \\implies \\log_{3} k &lt; \\lg 7\\cr     \\implies k &lt; 21.8\\cr     \\implies \\max(k) = 21\\cr     \\text{running time }= n^{\\log_{3} 21}\\cr \\end{aligned} $$</p>"},{"location":"Chap04/4.2/#42-4","title":"4.2-4","text":"<p>V. Pan discovered a way of multiplying $68\\times 68$ matrices using 132,464 multiplications, a way of multiplying $70\\times 70$ matrices using 143,640 multiplications, and a way of multiplying $72\\times 72$ matrices using 155,424 multiplications. Which method yields the best asymptotic running time when used in a divide-and-conquer matrix-multiplication algorithm? How does it compare with Strassen\u2019s algorithm?</p> <p>$n^{\\log_{68}132464}=n^{2.79513};n^{\\log_{70} 143640}=n^{2.79512};n^{\\log_{72} 155424}=n^{2.79515};n^{\\lg 7}=n^{2.80735}$</p> <p>the second method is the best; better than Strassen's algorithm.</p>"},{"location":"Chap04/4.2/#42-5","title":"4.2-5","text":"<p>Show how to multiply the complex numbers $a+bi$ and $c+di$ using only three multiplications of real numbers. The algorithm should take $a,b,c$, and $d$ as input and produce the real component $ac-bd$ and the imaginary component $ad+bc$ separately.</p> <p>$$ \\begin{aligned}     S_1=a(c-d)=ac-ad\\cr     S_2=b(c+d)=bc+bd\\cr     S_3=d(a-b)=ad-bd\\cr     S_1+S_3=ac-bd\\cr     S_2+S_3=ad+bc\\cr \\end{aligned} $$</p>"},{"location":"Chap04/4.2/#42-6","title":"4.2-6","text":"<p>Suppose that you have a $\\Theta(n^{\\alpha})$-time algorithm for squaring $n \\times n$ matrices, where $\\alpha \\geq 2$. Show how to use that algorithm to multiply two different $n \\times n$ matrices in $\\Theta(n^{\\alpha})$ time.</p> <p>$$ \\begin{aligned}     A\\cdot B=\\frac{(A+B)^{2}-(A-B)^2}{4} \\end{aligned} $$</p>"},{"location":"Chap04/4.3/","title":"4.3 The substitution method for solving recurrences","text":""},{"location":"Chap04/4.3/#43-1","title":"4.3-1","text":"<p>Use the substitution method to show that each of the following recurrences defined on the reals has the asymptotic solution specified:</p> <p>a. $T(n)=T(n-1)+n$ has solution $T(n) = O(n^2)$</p> <p>b. $T(n)=T(n/2)+\\Theta(1)$ has solution $T(n)=O(\\lg n)$</p> <p>c. $T(n)=2T(n/2)+n$ has solution $T(n)=O(n\\lg n)$.</p> <p>d. $T(n)=2T(n/2+17)+n$ has solution $T(n) = O(n\\lg n)$.</p> <p>e. $T(n)=2T(n/3)+\\Theta(n)$ has solution $T(n)=\\Theta(n)$</p> <p>f. $T(n) = 4T(n/2)+\\Theta(n)$ has solution $T(n)=\\Theta(n^2)$.</p> <p>a.</p> <p>$$ \\begin{aligned}     \\text{ inductive case: } &amp;\\cr     &amp; \\text{assume }T(n')\\leq cn'^2 \\text{ for all } n_0 \\leq n'\\leq n\\cr     T(n)&amp; =T(n-1)+n\\cr     &amp; \\leq c(n-1)^2+n\\cr     &amp; = cn^2+(1-2c)n+c\\cr     &amp; \\leq cn^2\\cr     &amp; \\text{while the last step holds for }c &gt; \\frac{1}{2}\\cr     \\text{base case: } &amp;\\cr     T(n) &amp; \\leq c \\leq cn \\text{ for all }n \\leq n_0 \\text{ is true if and only if choose a large enough } n_0\\cr \\end{aligned} $$</p> <ul> <li>tips: base case and detail like $\\text{ for all } n_0 \\leq n'\\leq n$ will be omitted in this kind of problems for convenience.</li> </ul> <p>b.</p> <p>assume  $T(n) \\leq c\\lg n$</p> <p>$$ \\begin{aligned}     T(n)&amp;=T(n/2)+c'\\cr     &amp;\\leq c\\lg n -c\\lg 2 +c'\\cr     &amp;\\leq c\\lg n\\cr \\end{aligned} $$</p> <p>the last step holds for $c&gt;c'/\\lg 2$.</p> <p>c.</p> <p>assume $T(n)\\leq cn\\lg n$</p> <p>$$ \\begin{aligned}     T(n) &amp; = 2T(n/2)+n\\cr     &amp; \\leq 2c(n/2)\\lg (n/2) +n\\cr     &amp; = cn\\lg n + (1-c\\lg 2)n\\cr     &amp; \\leq cn\\lg n \\end{aligned} $$</p> <p>the last step holds for $c&gt;1/\\lg 2$.</p> <p>d.</p> <p>assume $T(n)\\leq c(n-a)\\lg (n-a)$</p> <p>$$ \\begin{aligned}     T(n)&amp;=2T(n/2+17)+n\\cr     &amp; \\leq 2(c(n/2+17-a))\\lg(n/2-a+17)+n\\cr     &amp; = c(n-2a+34)\\lg(n -a +34)+(1-c\\lg 2)n +c(-2a+34)\\lg 2\\cr     &amp; \\leq c(n-a)\\lg (n-a)\\cr \\end{aligned} $$</p> <p>the last step holds for $n\\geq 34$</p> <p>e. assume $c_{1}n\\leq T(n)\\leq c_{2}n$</p> <p>$$ \\begin{aligned}     T(n) &amp; =2T(n/3)+\\Theta(n)\\cr     &amp; \\geq 2(c_{1}n/3)+c_{3}n\\cr     &amp; =(2c_{1}/3+c_{3})n\\cr     &amp; \\geq c_{1}n\\cr \\end{aligned} $$</p> <p>the last step holds for $c_{1}\\leq 3c_{3}$</p> <p>$c_{3}$ is the lowwer bound constant in $\\Theta(n)$.</p> <p>$$ \\begin{aligned}     T(n) &amp; =2T(n/3)+c_{4}n\\cr     &amp; \\leq (2c_{2}/3+c_{4})n\\cr     &amp; \\leq c_{2}n\\cr \\end{aligned} $$</p> <p>the last step holds for$c_{2}\\geq 3c_{4}$</p> <p>$c_{4}$ is the upper bound constant in $\\Theta(n)$.</p> <p>f.</p> <p>assume $c_{1}n^2 \\leq T(n)\\leq c_{2}n^2-c_{0}n$</p> <p>$$ \\begin{aligned}     T(n) &amp; =4T(n/2)+\\Theta(n)\\cr     &amp; \\geq c_{1}n^2+c_{3}n\\cr     &amp; \\geq c_{1}n^2\\cr     T(n) &amp; =4T(n/2)+\\Theta(n)\\cr     &amp; \\leq c_{2}n^2-2c_{0}n+c_{4}n\\cr     &amp; \\leq c_{2}n^2-c_{0}n\\cr \\end{aligned} $$</p> <p>the last step holds for $c_{0}\\geq c_{4}$</p> <p>$c_{3}$ is the lowwer bound constant in $\\Theta(n)$ and $c_{4}$ is the upper bound constant in $\\Theta(n)$.</p>"},{"location":"Chap04/4.3/#43-2","title":"4.3-2","text":"<p>The solution to the recurrence $T(n) = 4T(n/2) + n$ turns out to be $T(n) = \\Theta(n^2)$.Show that a substitution proof with the assumption $T(n) \\leq cn^2$ fails. Then show how to subtract a lower-order term to make a substitution proof work.</p> <p>assume $T(n)\\leq cn^2$</p> <p>$$ \\begin{aligned}     T(n) &amp; =4T(n/2)+n\\cr     &amp; \\leq cn^2+n\\cr \\end{aligned} $$</p> <p>It can not further imply $T(n)\\leq cn^2+n$</p> <p>assume $T(n)\\leq c_{1}n^2-c_{2}n$</p> <p>$$ \\begin{aligned}     T(n) &amp; =4T(n/2)+n\\cr     &amp; \\leq c_{1}n^2-2c_{2}n+n\\cr     &amp; \\leq c_{1}n^2-c_{2}n\\cr \\end{aligned} $$</p> <p>the last step holds for $c_{2}\\geq 1$</p>"},{"location":"Chap04/4.3/#43-3","title":"4.3-3","text":"<p>The recurrence $T(n)=2T(n-1)+1$ has the solution T(n) = O(2^n). Show that a subtitution proof fails with the assumption $T(n)\\leq c2^n$, where $c &gt; 0$ is constant. Then show how to subtract a lower-order term to make a substitution proof work.</p> <p>assume $T(n)\\leq c2^{n}$</p> <p>$$ \\begin{aligned}     T(n) &amp; =2T(n-1)+1\\cr     &amp; \\leq 2\\cdot c2^{n-1}+1\\cr     &amp; =c2^{n}+1\\cr \\end{aligned} $$</p> <p>It can not further imply $T(n)\\leq c2^{n}$</p> <p>assume $T(n)\\leq c2^{n}-d$</p> <p>$$ \\begin{aligned}     T(n) &amp; =2T(n-1)+1\\cr     &amp; \\leq 2\\cdot c2^{n-1}-2d+1\\cr     &amp; =c2^{n}-d\\cr \\end{aligned} $$</p> <p>the last step holds for $d\\geq 1$</p>"},{"location":"Chap04/4.4/","title":"4.4 The recursion-tree method for solving recurrences","text":""},{"location":"Chap04/4.4/#44-1","title":"4.4-1","text":"<p>For each of the following recurrences, sketch its recursion tree, and guess a good asymptotic upper bound on its solution. Then use the substitution method to verify your answer.</p> <p>a. $T(n)=T(n/2)+n^3$.</p> <p>b. $T(n) = 4T(n/3)+n$.</p> <p>c. $T(n) = 4T(n/2)+n$.</p> <p>d. $T(n) = 3T(n-1)+1$.</p> <p>a.</p> <p></p> <p>guess $T(n)=O(n^3)$,assume $T(n)\\leq cn^3$</p> <p>$$ \\begin{aligned}     T(n) &amp; = T(n/2)+n^3\\cr     &amp; \\leq \\frac{cn^3}{8}+n^3\\cr     &amp; \\leq (1+\\frac{c}{8})n\\cr     &amp; \\leq cn^3\\cr \\end{aligned} $$</p> <p>the last step holds for $c\\geq 7/8$</p> <p>b.</p> <p></p> <p>guess $T(n)=O(n^{\\log_{3} 4})$, assume $T(n)\\leq cn^{\\log_3 4}-dn$</p> <p>$$ \\begin{aligned}     T(n) &amp; =4T(n/3)+n\\cr     &amp; \\leq cn^{\\log_{3} 4} -\\frac{4dn}{3}+ n\\cr     &amp; \\leq cn^{\\log_{3} 4}\\cr \\end{aligned} $$</p> <p>the last step holds for $d\\geq 4/3$</p> <p>c.</p> <p></p> <p>guess $T(n)=O(n^2)$, assume $T(n)\\leq cn^2-dn$</p> <p>$$ \\begin{aligned}     T(n) &amp; =4T(n/2)+n\\cr     &amp; \\leq cn^2-2dn+n\\cr     &amp; \\leq cn^2\\cr \\end{aligned} $$</p> <p>the last step holds for $d\\geq 1/2$</p> <p>d.</p> <p></p> <p>guess $T(n)=O(3^n)$, assume $T(n)\\leq c3^n-d$</p> <p>$$ \\begin{aligned}     T(n) &amp; =3T(n-1)+1\\cr     &amp; \\leq 3\\cdot 3^{n-1}-3d +1\\cr     &amp; \\leq 3^n\\cr \\end{aligned} $$</p> <p>the last step holds for $d\\geq 1/3$</p>"},{"location":"Chap04/4.4/#44-2","title":"4.4-2","text":"<p>Use the substitution method to prove that recurrence (4.15) has the asymptotic lower bound $L(n)=\\Omega(n)$. Conclude that $L(n)=\\Theta(n)$.</p> <p>assume$L(n)\\geq cn$</p> <p>$$ \\begin{aligned}     L(n) &amp; =L(n/3)+L(2n/3)\\cr     &amp; \\geq c(n/3)+c(n2/3)\\cr     &amp; = cn\\cr \\end{aligned} $$</p> <p>since the textbook has proved that $L(n)=O(n)$, and now $L(n)=\\Omega(n)$ is proved, we can conclude that $L(n)=\\Theta(n)$.</p>"},{"location":"Chap04/4.4/#44-3","title":"4.4-3","text":"<p>Use the substitution method to prove that recurrence (4.14) has the solution $T(n) = \\Omega(n\\lg n)$. Conclude  that $T(n)=\\Theta(n\\lg n)$.</p> <p>guess $T(n)\\leq cn\\lg n$</p> <p>$$ \\begin{aligned}     T(n) &amp; =T(n/3)+T(2n/3)+\\Theta(n)\\cr     &amp; \\leq c(n/3)\\lg(n/3)+c(2n/3)\\lg(2n/3)+dn\\cr     &amp; = cn\\lg{n}-\\frac{cn}{3}(3\\lg{3}-2-3d/c)\\cr     &amp; \\leq cn\\lg{n}\\cr \\end{aligned} $$</p> <p>the last step holds for $c\\geq 3d/2$.</p>"},{"location":"Chap04/4.4/#44-4","title":"4.4-4","text":"<p>Use a recursion tree to justify a good guess for the solution to the recurrence $T(n)=T(\\alpha n)+T((1-\\alpha)n)+\\Theta(n)$, where $\\alpha$ is a constant in the range $0 &lt; \\alpha 1$.</p> <p>assume $\\alpha&lt;1/2$, since otherwise let $\\beta=1-\\alpha =$ and solve it for $\\beta$.</p> <p>let $L(n)$ donates leaves number, then $L(n)=L(\\alpha n)+L((1-\\alpha)n)\\implies L(n)=\\Theta(n)$</p> <p>the smallest depth of leaves is $\\log_{1/\\alpha}n$, while the largest depth is $\\log_{\\alpha}n$, total cost over all nodes at depth $i$, for $i=0,1,\\dots,\\log_{1/\\alpha}n-1$ is $\\Theta(n)$</p> <p>$$ \\begin{aligned}     &amp; \\sum_{i=0}^{\\log_{1/\\alpha}n-1}c_{1}n+d_{1}n\\leq T(n)\\leq \\sum_{i=0}^{\\log_{\\alpha}n-1}c_{2}n+d_{2}n\\cr     \\implies &amp; c_{1}n\\log_{1/\\alpha}n+d_{1}n \\leq T(n) \\leq c_{2}n\\log_{\\alpha}n+d_{2}n\\cr     \\implies &amp; T(n)=\\Theta(n\\lg n)\\cr \\end{aligned} $$</p>"},{"location":"Chap04/4.5/","title":"4.5 The master method for solving recurrences","text":""},{"location":"Chap04/4.5/#45-1","title":"4.5-1","text":"<p>Use the master method to give tight asymptotic bounds for the following recurrences.</p> <p>a. $T(n)=2T(n/4)+1$.</p> <p>b. $T(n)=2T(n/4)+\\sqrt{n}$.</p> <p>c. $T(n)=2T(n/4)+\\sqrt{n}\\lg^2 n$.</p> <p>d. $T(n)=2T(n/4)+n$.</p> <p>e. $T(n)=2T(n/4)+n^2$.</p> <p>a.</p> <p>$$ \\begin{aligned}     a=2,b=4,n^{\\log_{b}a}=n^{1/2},f(n)=1=O(n^{\\log_{b}a-1/2})\\cr     T(n)=\\Theta(n^{1/2})\\cr \\end{aligned} $$</p> <p>b.</p> <p>$$ \\begin{aligned}     a=2,b=4,n^{\\log_{b}a}=n^{1/2},f(n)=n^{1/2}=\\Theta(n^{\\log_{b}a})\\cr     T(n)=\\Theta(n^{1/2}\\lg n)\\cr \\end{aligned} $$</p> <p>c.</p> <p>$$ \\begin{aligned}     a=2,b=4,n^{\\log_{b}a}=n^{1/2},f(n)=n^{1/2}\\lg^{2}n=\\Theta(n^{\\log_{b}a}\\lg^{2}n)\\cr     T(n)=\\Theta(n^{1/2}\\lg^{3} n)\\cr \\end{aligned} $$</p> <p>d.</p> <p>$$ \\begin{aligned}     a=2,b=4,n^{\\log_{b}a}=n^{1/2},f(n)=n=\\Omega(n^{\\log_{b}a+1/2})\\cr     \\text{regularity condition: }af(n/b)=n/2\\leq (1/2)n\\cr     T(n)=\\Theta(n)\\cr \\end{aligned} $$</p> <p>e.</p> <p>$$ \\begin{aligned}     a=2,b=4,n^{\\log_{b}a}=n^{1/2},f(n)=n^2=\\Omega(n^{\\log_{b}a+3/2})\\cr     \\text{regularity condition: }af(n/b)=n/8\\leq (1/8)n\\cr     T(n)=\\Theta(n^2)\\cr \\end{aligned} $$</p>"},{"location":"Chap04/4.5/#45-2","title":"4.5-2","text":"<p>Professor Caesar wants to develop a matrix-multiplication algorithm that is asymptotically faster than Strassen\u2019s algorithm. His algorithm will use the divide-and-conquer method, dividing each matrix into $n/4\\times n/4$ submatrices, and the divide and combine steps together will take $\\Theta(n^2)$ time. Suppose that the professor\u2019s algorithm creates a recursive subproblems of size $n/4$. What is the largest integer value of $a$ for which his algorithm could possibly run asymptotically faster than Strassen\u2019s?</p> <p>$$ \\begin{aligned}     n^{\\log_{b}a}=n^{\\log_{4}a} &lt; n^{\\lg 7}\\cr     \\implies a &lt; 49\\cr \\end{aligned} $$</p>"},{"location":"Chap04/4.5/#45-3","title":"4.5-3","text":"<p>Use the master method to show that the solution to the binary-search recurrence $T(n) = T(n/2) + \\Theta(1)$ is $T(n) = \\Theta(\\lg n)$. (See Exercise 2.3-6 for a description of binary search.)</p> <p>$$ \\begin{aligned}     n^{\\log_{b}a}=n^{\\log_{2}1}=1\\cr     f(n)=1=\\Theta(n^{\\log_{b}a}\\lg^{0}n)\\cr     T(n)=\\Theta(\\lg n)\\cr \\end{aligned} $$</p>"},{"location":"Chap04/4.5/#45-4","title":"4.5-4","text":"<p>Consider the function $f(n) = \\lg n$. Argue that although $f(n/2) &lt; f(n)$, the regularity condition $af(n/b)\\leq cf(n)$ with $a=1$ and $b=2$ does not hold for any constant $c&lt;1$. Argue further that for any $\\epsilon &gt; 0$, the condition in case 3 that $f(n) = \\Omega(n^{\\log_{b}a+\\epsilon})$ does not hold.</p> <p>$$ \\begin{aligned}     &amp;\\forall c&lt;1\\cr     &amp;f(n/2)\\leq cf(n)\\cr     \\iff &amp; \\lg n-1\\leq c\\lg n\\cr     \\iff &amp; \\lg n\\leq 1/(1-c)\\cr     \\iff &amp; n\\leq 2^{1/(1-c)}\\cr \\end{aligned} $$</p> <p>so the regularity condition does not hold for $n\\geq 2^{1/(1-c)}$</p> <p>$$ \\begin{aligned}     n^{\\log_{b}a}=n^{\\log_{2}1}=1\\cr     \\forall \\epsilon&gt;0\\cr     cn^{\\epsilon}\\leq \\lg n \\text{ does not hold for }n\\to \\infty\\cr \\end{aligned} $$</p> <p>so $f(n) = \\Omega(n^{\\log_{b}a+\\epsilon})$ does not hold.</p>"},{"location":"Chap04/4.5/#45-5","title":"4.5-5","text":"<p>Show that for suitable constants $a,b$, and $\\epsilon$, the function $f(n)=2^{\\lceil\\lg n\\rceil}$ satisfies all the conditions in case 3 of the master theorem except the regularity condition.</p> <p>$$ \\begin{aligned}     a=1.1,b=1.2,n^{\\log_{b}a}=n^{\\log_{1.2}1.1}\\cr     f(n)\\geq 2^{\\lg n}=n\\geq n^{\\log_{1.2}1.1+\\log_{1.2}1.001}\\cr     \\implies \\exist \\epsilon&gt;0 \\text{ such that }f(n)=\\Omega(n^{\\log_{b}a+\\epsilon})\\cr \\end{aligned} $$</p> <p>$f(n)$ satisfies all the conditions in case 3 of the master theorem except the regularity condition.</p> <p>$1.1f(\\frac{1.3\\cdot 2^k}{1.2})=1.1\\cdot 2^{k+1}\\leq cf(1.3\\cdot 2^k)=c\\cdot 2^{k+1}$ dose not hold for any c&lt;1.</p>"},{"location":"Chap04/4.6/","title":"$\\star$ 4.6 Proof of the continuous master theorem","text":""},{"location":"Chap04/4.6/#46-1","title":"4.6-1","text":"<p>Show that $\\sum_{j=0}^{\\lfloor \\log_{b}n\\rfloor}(\\log_{b} n-j)^k=\\Omega(\\log_b^{k+1}n)$.</p> <p>$$ \\begin{aligned}     \\sum_{j=0}^{\\lfloor \\log_{b}n\\rfloor}(\\log_{b} n-j)^k &amp; \\geq \\sum_{j=0}^{\\lfloor \\log_{b}n\\rfloor}(\\lfloor \\log_{b}n\\rfloor n-j)^k\\cr     &amp; = \\sum_{j=0}^{\\lfloor \\log_{b}n\\rfloor}(j)^k\\cr     &amp; \\geq \\sum_{j=0}^{\\log_{b}n-1}(j)^k\\cr     &amp; = \\Theta((\\log_{b}n-1)^{k+1})\\cr     &amp; = \\Omega(\\log_b^{k+1}n)\\cr \\end{aligned} $$</p>"},{"location":"Chap04/4.6/#star-46-2","title":"$\\star$ 4.6-2","text":"<p>Show that case 3 of the master theorem is overstated (which is also why case 3 of Lemma 4.3 does not require that $f(n)=\\Omega(n^{\\log_b a+\\epsilon})$) in the sense that the regularity condition $af(n/b)\\leq cf(n)$ for some constant $c&lt;1$ implies that there exists a constant $\\epsilon &gt; 0$ sunch that $f(n)=\\Omega(n^{\\log_b a+\\epsilon})$.</p> <p>$$ \\begin{aligned}     &amp; af(n/b) \\leq cf(n)\\cr     \\implies &amp; f(n)\\geq \\frac{a}{c}f(n/b)\\cr     \\implies &amp; f(n) \\geq (\\frac{a}{c})^i f(n/{b^i})\\cr     &amp; \\text{let } n=b^i\\cr     \\implies &amp; f(n)\\geq (\\frac{a}{c})^{\\log_{b}n}f(1)\\cr     \\implies &amp; f(n) \\geq n^{\\log_{b}\\frac{a}{c}}\\cr     &amp; \\because c&lt;0,\\therefore \\frac {a}{c}=a+\\epsilon \\text{ for some constant }\\epsilon &gt;0\\cr     \\implies &amp; f(n)=\\Omega(n^{\\log_{b}a+\\epsilon})\\cr \\end{aligned} $$</p>"},{"location":"Chap04/4.6/#star-46-3","title":"$\\star$ 4.6-3","text":"<p>For $f(n)=\\Theta(n^{\\log_b a}/\\lg n)$, prove that the summation in equation (4.19) has solution $g(n)=\\Theta(n^{\\log_b{a}}\\lg\\lg n)$. Conclude that a master recurrence $T(n)$ using $f(n)$ as its driving function has solution $T(n) = \\Theta(n^{\\log_b a}\\lg\\lg n)$.</p> <p>$$ \\begin{aligned}     &amp; \\text{let }n=b^i\\cr     g(n) = &amp; \\sum_{j=0}^{i}a^jf(n/b^j)\\cr     = &amp; \\sum_{j=0}^{i}a^jf(b^{i-j})\\cr     = &amp; \\sum_{j=0}^{i}a^j\\Theta(a^{i-j}/(i-j))\\cr     = &amp; \\sum_{j=0}^{i}a^i\\Theta(1/j)\\cr     = &amp; a^i\\Theta(\\lg i)\\cr     = &amp; \\Theta(a^{\\log_{b}n} \\lg\\log_{b}n)\\cr     = &amp; \\Theta(n^{\\log_{b}a} \\lg\\lg n)\\cr     g(bn) = &amp; \\Theta((bn)^{\\log_{b}a} \\lg\\lg bn)=(n^{\\log_{b}a} \\lg\\lg n)\\cr     \\implies g(n)=&amp; (n^{\\log_{b}a} \\lg\\lg n)\\text{ even if }n \\neq b^i\\cr \\end{aligned} $$</p> <p>We can concluded that $T(n)=g(n)+n^{\\log_{b}a}=n^{\\log_{b}a} \\lg\\lg n$.</p>"},{"location":"Chap04/4.7/","title":"$\\star$ 4.7 Akra-Bazzi recurrences","text":""},{"location":"Chap04/4.7/#star-47-1","title":"$\\star$ 4.7-1","text":"<p>Consider an Akra-Bazzi recurrence $T(n)$ on the reals as given in recurrence (4.22), and define $T'(n)$ as</p> <p>$$ \\begin{aligned}   T'(n)=cf(n)+\\sum_{i=1}^k a_iT'(n/b_i), \\end{aligned} $$</p> <p>where $c &gt; 0$ is constant. Prove that whatever the implicit initial conditions for T(n) might be, there exist initial conditions for $T'(n)$ such that $T'(n) = cT(n)$ for all $n&gt;0$. Conclude that we can drop the asymptotics on a driving function in any Akra-Bazzi recurrence without affecting its asymptotic solution.</p>"},{"location":"Chap04/4.7/#47-2","title":"4.7-2","text":"<p>Show that $f(n)=n^2$ satisfies the polynomial-growth condition but that $f(n)=2^n$ does not.</p>"},{"location":"Chap04/4.7/#47-3","title":"4.7-3","text":"<p>Let f(n) be a function that satisfies the polynomial-growth condition. Prove that $f(n)$ is asymptotically positive, that is, there exists a constantis asymptotically positive, that is, there exists a constant $n_0\\geq 0$ such that $f(n)\\geq 0$ for all $n\\geq n_0$.</p>"},{"location":"Chap04/4.7/#star-47-4","title":"$\\star$ 4.7-4","text":"<p>Give an example of a function $f(n)$ that does not satisfy the polynomial-growth condition but for which $f(\\Theta(n)) = \\Theta(f(n))$.</p>"},{"location":"Chap04/4.7/#47-5","title":"4.7-5","text":"<p>Use the Akra-Bazzi method to solve the following recurrences.</p> <p>a. $T(n) = T(n/2)+T(n/3)+T(n/6)+n\\lg n$.</p> <p>b. $T(n) = 3T(n/3)+8T(n/4)+n^2/\\lg n$.</p> <p>c. $T(n) = (2/3)T(n/3)+(1/3)T(2n/3)+\\lg n$.</p> <p>d. $T(n) = (1/3)T(n/3) + 1/n$.</p> <p>e. $T(n) = 3T(n/3) + 3T(2n/3)+n^2$.</p>"},{"location":"Chap04/4.7/#star-47-6","title":"$\\star$ 4.7-6","text":"<p>Use the Akra-Bazzi method to prove the continuous master theorem.</p>"},{"location":"Chap04/Problems/4-1/","title":"4-1 Recurrence examples","text":"<p>Give asymptotically tight upper and lower bounds for $T(n)$ in each of the following algorithmic recurrences. Justify your answers.</p> <p>a. $T(n) = 2T(n/2)+n^3$.</p> <p>b. $T(n) = T(8n/11)+n$.</p> <p>c. $T(n) = 16T(n/4)+n^2$.</p> <p>d. $T(n)=4T(n/2)+n^2\\lg n$.</p> <p>e. $T(n) = 8T(n/3)+n^2$.</p> <p>f. $T(n)=7T(n/2)+n^2\\lg n$.</p> <p>g. $T(n)=2T(n/4)+\\sqrt{n}$.</p> <p>h. $T(n)=T(n-2)+n^2$.</p>"},{"location":"Chap04/Problems/4-2/","title":"4-2 Parameter-passing costs","text":"<p>Throughout this book, we assume that parameter passing during procedure calls takes constant time, even if an N -element array is being passed. This assumption is valid in most systems because a pointer to the array is passed, not the array itself. This problem examines the implications of three parameter-passing strategies:</p> <ol> <li> <p>Arrays are passed by pointer. Time $= \\Theta(1)$.</p> </li> <li> <p>Arrays are passed by copying. Time $=\\Theta(1)$, where $N$ is the size of the array.</p> </li> <li> <p>Arrays are passed by copying only the subrange that might be accessed by the called procedure. Time $=\\Theta(n)$ if the subarray contains $n$ elements.</p> </li> </ol> <p>Consider the following three algorithms:</p> <p>a. The recursive binary-search algorithm for finding a number in a sorted array(see Exercise 2.3-6).</p> <p>b. The MERGE-SORT procedure from Section 2.3.1.</p> <p>c. The MATRIX-MULTIPLY-RECURSIVE procedure from Section 4.1.</p> <p>Give nine recurrences $T_{a1}(N,n),T_{a2}(N,n),\\dots ,T_{c3}(T,n)$ for the worst-case running times of each of the three algorithms above when arrays and matrices are passed using each of the three parameter-passing strategies above. Solve your recurrences, giving tight asymptotic bounds.</p>"},{"location":"Chap04/Problems/4-3/","title":"4-3 Solving recurrences with a change of variables","text":"<p>Sometimes, a little algebraic manipulation can make an unknown recurrence similar to one you have seen before. Let\u2019s solve the recurrence</p> <p>$T(n)=2T(\\sqrt{n})+\\Theta(\\lg n)$</p> <p>by using the change-of-variables method.</p> <p>a. Define $m=\\lg n$ and $S(m)=T(2^m)$. Rewrite recurrence (4.25) in terms of m and $S(m)$.</p> <p>b. Solve your recurrence for $S(m)$.</p> <p>c. Use your solution for $S(m)$ to conclude that $T(n)=\\Theta(\\lg n\\lg\\lg n)$.</p> <p>d. Sketch the recursion tree for recurrence (4.25), and use it to explain intuitively why the solution is $T(n)=\\Theta(\\lg n \\lg\\lg n)$.</p> <p>Solve the following recurrences by changing variables:</p> <p>e. $T(n)=2T(\\sqrt{n})+\\Theta(1)$.</p> <p>f. $T(n)=3T(\\sqrt[3]{n})+\\Theta(n)$.</p>"},{"location":"Chap04/Problems/4-4/","title":"4-4 More recurrence examples","text":"<p>Give asymptotically tight upper and lower bounds for T(n) in each of the following recurrences. Justify your answers.</p> <p>a. $T(n)=5T(n/3)+n\\lg n$.</p> <p>b. $T(n)=3T(n/3)+n/\\lg n$.</p> <p>c. $T(n)= 8T(n/2)+n^3\\sqrt{n}$.</p> <p>d. $T(n)=2T(n/2-2)+n/2$.</p> <p>e. $T(n)=2T(n/2)+n/\\lg n$.</p> <p>f. $T(n)=T(n/2)+T(n/4)+T(n/8)+n$.</p> <p>g. $T(n)=T(n-2)+1/n$.</p> <p>h. $T(n)=T(n-1)+\\lg n$.</p> <p>i. $T(n)=T(n-2)+1/\\lg n$.</p> <p>j. $T(n)=\\sqrt{n}T(\\sqrt{n})+n$.</p>"},{"location":"Chap04/Problems/4-5/","title":"4-5 Fibonacci numbers","text":"<p>This problem develops properties of the Fibonacci numbers, which are defined by recurrence (3.31) on page 69. We\u2019ll explore the technique of generating functions to solve the Fibonacci recurrence. Define the generating function (or formal power series) $\\mathcal{F}$ as</p> <p>$$ \\begin{aligned} \\mathcal{F} &amp; = \\sum_{i=0}^{\\infty}F_iz^i\\cr &amp; = 0+z+z^2+2z^3+3z^4+5z^5+8z^6+13z^7+21z^8+\\cdots, \\end{aligned} $$ where $F_i$ is the $i$th Fibonaccci nunber.</p> <p>a. Show that $\\mathcal{F}(z)=z+z\\mathcal{F}(z)+z^2\\mathcal{F}(z)$.</p> <p>b. Show that</p> <p>$$ \\begin{aligned} \\mathcal{F}(z) &amp; = \\frac{z}{1-z-z^2}\\cr &amp; = \\frac{z}{(1-\\phi z)(1-\\hat\\phi z)} &amp; = \\frac{1}{\\sqrt{5}}(\\frac{1}{1-\\phi z}-\\frac{1}{1-\\hat\\phi z}), \\end{aligned} $$</p> <p>where $\\phi$ is the golden ratio, and $\\hat{\\phi}$ is its conjugatr(see page 69).</p> <p>c. Show that</p> <p>$$ \\mathcal{F}(z)=\\sum_{i=0}^{\\infty}\\frac{1}{\\sqrt{5}}(\\phi^{i}-\\hat{\\phi}^i)z^i. $$</p> <p>You may use without proof the generating-function version of equation (A.7) on page 1142, $\\sum_{k=0}^{\\infty} x^k=1/(1-x)$. Because this equation involves a generating function, $x$ is formal variable, not a real-valued variable, so that you don\u2019t have to worry about convergence of the summation or about the requirement in equation (A.7) that $|x|&lt;1$, which doesn\u2019t make sense here.</p> <p>d. Use part (c) to prove that $F_i=\\phi^i/\\sqrt{5}$ for $i&gt;0$, rounded  to the nearest integer.</p> <p>(Hint: Observe that $|\\hat\\phi&lt;1|$.)</p> <p>e Prove that $F_{i+2}\\geq \\phi^{i}$ for $i\\geq 0$.</p>"},{"location":"Chap04/Problems/4-6/","title":"4-6 Chip testing","text":"<p>Professor Diogenes has n supposedly identical integrated-circuit chips that in principle are capable of testing each other. The professor\u2019s test jig accommodates two chips at a time. When the jig is loaded, each chip tests the other and reports whether it is good or bad. A good chip always reports accurately whether the other chip is good or bad, but the professor cannot trust the answer of a bad chip. Thus, the four possible outcomes of a test are as follows:</p> Chip A asys Chip B asys Chip C asys B is good A is good both are good, or both are bad B is good A is bad at least one is bad B is bad A is good at least one is bad B is bad A is bad at least one is bad <p>a. Show that if at least $n=2$ chips are bad, the professor cannot necessarily determine which chips are good using any strategy based on this kind of pairwise test. Assume that the bad chips can conspire to fool the professor.</p> <p>Now you will design an algorithm to identify which chips are good and which are bad, assuming that more than $n=2$ of the chips are good. First, you will determine how to identify one good chip.</p> <p>b. Show that $\\lfloor n/2 \\rfloor$ pairwise tests are sufficient to reduce the problem to one of nearly half the size. That is, show how to use $\\lfloor n/2 \\rfloor$ pairwise tests to obtain a set with at most $\\lceil n/2 \\rceil$ chips that still has the property that more than half of the chips are good.</p> <p>c. Show how to apply the solution to part (b) recursively to identify one good chip. Give and solve the recurrence that describes the number of tests needed to identify one good chip.</p> <p>You have now determined how to identify one good chip.</p> <p>d. Show how to identify all the good chips with an additional $\\Theta(n)$ pairwise tests.</p>"},{"location":"Chap04/Problems/4-7/","title":"4-7 Monge arrays","text":"<p>An $,\\times n$ array $A$ of real numbers is a Monge array if for all $i,j,k$, and $l$ such that $1\\leq i &lt;k \\leq m$ and $i \\leq j &lt; l \\leq n$, we have</p> <p>$A[i,j]+A[k,l]\\leq A[i,l]+A[k,j]$.</p> <p>In other words, whenever we pick two rows and two columns of a Monge array and consider the four elements at the intersections of the rows and the columns, the sum consider the four elements at the intersections of the rows and the columns, the sum lower-left and upper-right elements. For example, the following array is Monge:</p> <p>$$ \\begin{array}{}     10 &amp; 17 &amp; 13 &amp; 28 &amp; 23\\cr     17 &amp; 22 &amp; 16 &amp; 29 &amp; 23\\cr     24 &amp; 28 &amp; 22 &amp; 34 &amp; 24\\cr     11 &amp; 13 &amp; 6 &amp; 17 &amp; 7\\cr     45 &amp; 44 &amp; 32 &amp; 37 &amp; 23\\cr     36 &amp; 33 &amp; 19 &amp; 21 &amp; 6\\cr     75 &amp; 66 &amp; 51 &amp; 53 &amp; 34\\cr \\end{array} $$</p> <p>a. Prove that an array is Monge if and only if for all $i=1,2,\\dots,m-1$ and $j=1,2,\\dots,n-1$, we have</p> <p>$$ A[i,j]+A[i+1,j+1]\\leq A[i,j+1]+A[i+1,j]. $$</p> <p>(Hint: For the \"if\" part, use induction separately on rows and columns.)</p> <p>b. The following array is not Monge. Change one element in order to make it Monge. (Hint: Use part (a).)</p> <p>$$ \\begin{array}{} 37 &amp; 23 &amp; 22 &amp; 32\\cr 21 &amp; 6 &amp; 7 &amp; 10\\cr 53 &amp; 34 &amp; 30 &amp; 31\\cr 32 &amp; 13 &amp; 9 &amp; 6\\cr 43 &amp; 21 &amp; 15 &amp; 8\\cr \\end{array} $$</p> <p>c. Let $f(i)$ be the index of the column containing the leftmost minimum elementof row $i$. Prove that $f(1)\\leq f(2)\\leq \\cdots \\leq f(m)$ for any $m\\times n$ Monge array.</p> <p>d. Here is a description of a divide-and-conquer algorithm that computes the left most minimum element in each row of an $m\\times n$ Monge array A:</p> <p>Construct a submatrix $A'$ of $A$ consisting of the even-numbered rows of $A$. Recursively determine the leftmost minimum for each row of $A'$. Then compute the leftmost minimum in the odd-numbered rows of $A'$.</p> <p>Explain how to compute the leftmost minimum in the odd-numbered rows of $A$ (given that the leftmost minimum of the even-numbered rows is known) in $O(m+n) time.</p> <p>e. Write the recurrence for the running time of the algorithm in part (d). Show that its solution is $O(m+n\\log m)$.</p>"}]}